{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"iceOS Documentation","text":"<p>Welcome to the iceOS docs!  This site provides comprehensive guidance for building, orchestrating and deploying AI-powered workflows with ScriptChain, Nodes, Agents and Tools.</p> <p>Use the navigation on the left (or the \u2191 menu on mobile) to browse.</p>"},{"location":"#who-is-this-for","title":"Who is this for?","text":"<ul> <li>Developers who want to embed AI workflows in their applications.</li> <li>Data scientists prototyping chains locally before scaling.</li> <li>DevOps teams deploying the FastAPI server to production.</li> </ul> <p>If you are just getting started, head over to the Quick Start. </p>"},{"location":"EXTERNAL_PITCH/","title":"iceOS \u2013 The AI-Native Operating Layer for Agentic Workflows","text":"<p>Build, run &amp; scale reliable multi-agent AI systems in minutes.</p>"},{"location":"EXTERNAL_PITCH/#1-the-problem","title":"1. The Problem","text":"<p>Modern AI products require orchestrating many moving parts: calling multiple LLMs, deterministic tools, conditional logic, retries, auditing, cost control and more.  Stitching this together from scratch means gluing SDKs, schedulers and guard-rails \u2013 every team re-implements the same boilerplate and still ends up with brittle pipes.</p>"},{"location":"EXTERNAL_PITCH/#2-the-iceos-solution","title":"2. The iceOS Solution","text":"<p>iceOS is an open-source operating layer that turns these moving parts into a declarative workflow DAG executed by an async engine with built-in guard-rails.</p> <ul> <li>Composable Nodes \u2013 Ingress (webhook), LLM, Tool, Condition, Agent, Sink.</li> <li>Type-Safe SDK \u2013 Pydantic-powered contracts catch integration bugs before runtime.</li> <li>Deep Guard-Rails \u2013 depth, token &amp; semantic limits plus pluggable policy hooks.</li> <li>Level-Parallel Executor \u2013 maximises concurrency without violating dependencies.</li> <li>Self-Optimising Runtime \u2013 telemetry loop learns &amp; rewrites flows for latency/cost.</li> </ul> <p>Result: teams focus on product logic, not plumbing.</p>"},{"location":"EXTERNAL_PITCH/#3-what-can-you-build-today","title":"3. What Can You Build Today?","text":"Use-Case Nodes Used Outcome Weekly Research Digest Webhook \u2192 FileSearch \u2192 LLM (summarise) \u2192 Email PDF folder summarised &amp; mailed every Friday GitHub Issue Triager GitHub Ingress \u2192 Agent \u2192 Label Tool Issues labelled &amp; prioritised in seconds Slack Knowledge Bot Slack \u2192 Agent (LLM + WebSearch) \u2192 Slack Org-wide Q&amp;A with source citations ETL Data Pipeline SQL Fetch \u2192 Condition \u2192 Validator \u2192 S3 Sink Valid, versioned dataset ready for BI"},{"location":"EXTERNAL_PITCH/#4-architecture-high-level","title":"4. Architecture (High-Level)","text":"<pre><code>graph TD\n  UI[\"CLI / Canvas / API\"] --&gt; ORCH[\"Async Orchestrator\"]\n  ORCH --&gt; SDK[\"Type-Safe SDK\"]\n  SDK --&gt; NODES[\"Modular Nodes\"]\n  ORCH --&gt; CACHE[\"Result Cache\"]\n  ORCH --&gt; METRICS[\"Telemetry\"]\n  NODES --&gt; EXT[\"External APIs &amp; LLM Providers\"]\n</code></pre>"},{"location":"EXTERNAL_PITCH/#5-key-differentiators","title":"5. Key Differentiators","text":"<ol> <li>Guard-Rails First \u2013 Enterprise-grade policies baked in; not an after-thought.</li> <li>Pythonic DX \u2013 IDE auto-completion, Ruff, MyPy &amp; Pyright enforced out of the box.</li> <li>Async Level Parallelism \u2013 &lt;40 \u00b5s overhead per node; scales to hundreds of tools.</li> <li>Plug-in Ecosystem \u2013 Discover &amp; install nodes with <code>ice sdk create-node</code>.</li> </ol>"},{"location":"EXTERNAL_PITCH/#6-roadmap-snapshot-h2-2025","title":"6. Roadmap Snapshot (H2 2025)","text":"Quarter Milestone Highlights Q3 \u2192 v0.3 Frozen Public SDK Manylinux wheels, generated API docs Q3 Developer CLI GA <code>ice new</code>, hot-reload, auto-registration Q4 Frosty Copilot \u03b1 NL canvas that generates live workflows Q4 Marketplace \u03b1 Verified node registry &amp; auto-update"},{"location":"EXTERNAL_PITCH/#7-quick-start-2-min","title":"7. Quick Start (2 min)","text":"<pre><code># Clone &amp; enter repo\n$ git clone https://github.com/stef-writes/iceOS.git &amp;&amp; cd iceOS\n\n# Install deps &amp; run demo chain\n$ python -m venv .venv &amp;&amp; source .venv/bin/activate\n$ pip install -e .[dev] &amp;&amp; python scripts/demo_run_chain.py\n</code></pre>"},{"location":"EXTERNAL_PITCH/#8-community-support","title":"8. Community &amp; Support","text":"<p>\u2022 Slack \u2013 join <code>ice-community.slack.com</code> for questions &amp; pairing. \u2022 GitHub Discussions \u2013 roadmap input &amp; showcase your flows. \u2022 Commercial Support \u2013 email <code>team@iceos.ai</code> for SLA packages.</p>"},{"location":"EXTERNAL_PITCH/#9-licence","title":"9. Licence","text":"<p>iceOS is MIT-licensed \u2013 free for personal &amp; commercial use.  We welcome contributions via issues &amp; pull requests! </p>"},{"location":"accelerator_application/","title":"Accelerator application","text":""},{"location":"accelerator_application/#iceos-accelerator-application","title":"iceOS \u2013 Accelerator Application","text":"<p>Date: </p>"},{"location":"accelerator_application/#1-one-sentence-pitch","title":"1. One-sentence pitch","text":"<p>iceOS is a drag-and-drop \"AI control room\" where teams snap together ready-made skills on an infinite canvas to automate their work in minutes.</p>"},{"location":"accelerator_application/#2-product-service-description","title":"2. Product / service description","text":"<p>iceOS combines four tightly-integrated modules: 1. Infinite Canvas UI \u2013 an endless, real-time board (think Miro) where users drag skills like Search, Summarise, Email or custom tools onto the canvas and connect them visually. 2. Frosty AI Assistant \u2013 an on-board co-pilot that watches the canvas, asks Socratic questions and can auto-generate or re-wire workflows on demand. 3. Safety-First Execution Engine \u2013 an open-source backend that runs those automations in parallel, enforces budget &amp; content guard-rails and learns from every run to cut cost and latency next time. 4. Emergent Workflow Co-Creation Framework \u2013 our conceptual backbone that keeps ideas fuzzy until the user is ready to materialise them.  It supports ambiguity-preserving workspaces, progressive materialisation (idea \u2192 stub \u2192 executable), agentic swarm orchestration and a living artifact continuum that ties design history to runtime metrics. The result: business users design workflows as easily as sketching, while engineers get a reliable, auditable runtime under the hood.</p>"},{"location":"accelerator_application/#3-problem-we-solve","title":"3. Problem we solve","text":"<p>Building AI workflows today is messy and slow\u2014developers juggle prompt engineering, error handling, rate-limits and multiple APIs, while non-technical teams are locked out entirely.  The overhead steals weeks per project and risks runaway costs.</p>"},{"location":"accelerator_application/#4-why-we-are-passionate","title":"4. Why we are passionate","text":"<p>Each founder has lived this pain: we spent months wiring brittle scripts at Stripe, Notion and health-tech startups.  We want the next generation to focus on customer value, not plumbing.</p>"},{"location":"accelerator_application/#5-mission-core-values","title":"5. Mission &amp; core values","text":"<p>Mission: Make powerful AI automation as simple as sketching on a whiteboard.</p> <p>Core values: - Openness (MIT licence, open roadmap) - Reliability (safety first, data-driven) - Developer-love (DX tools, clear docs) - Measurable impact (cost &amp; time savings)</p>"},{"location":"accelerator_application/#6-customers","title":"6. Customers","text":"<ul> <li>Today: AI engineers and operations teams at Series-A/B SaaS firms using our private-alpha canvas.</li> <li>12 mo: Agencies / consultancies packaging automations for clients.</li> <li>36 mo: \"Citizen developers\" inside mid-market companies.</li> </ul>"},{"location":"accelerator_application/#7-competitors","title":"7. Competitors","text":"<ul> <li>Now: Workflow86, LangFlow, n8n (with AI nodes).</li> <li>At scale: Zapier AI, Microsoft Copilot Studio, ServiceNow Flow Designer.</li> </ul>"},{"location":"accelerator_application/#8-our-advantage","title":"8. Our advantage","text":"<ul> <li>Built-in safety nets (budget &amp; content filters) absent in visual rivals.</li> <li>Frosty AI co-pilot drafts flows automatically\u2014saves hours.</li> <li>Open-source core \u2192 on-prem deployment for regulated sectors.</li> <li>Self-optimising engine reduces run cost ~20 % over time.</li> <li>Emergent Workflow Co-Creation Framework lets users postpone commitment and iterate at any fidelity\u2014no rival offers this flexibility.</li> </ul>"},{"location":"accelerator_application/#9-team-qualifications","title":"9. Team qualifications","text":"<ul> <li>Stef \u2014 CEO/CTO: ex-Stripe infra engineer, led async platform handling 50 M req/day.</li> <li>Ada \u2014 Lead Engineer: creator of open-source tools with 15 k GitHub stars.</li> <li>Jordan \u2014 Head of Product: former Notion PM specialised in visual collaboration UX.</li> </ul>"},{"location":"accelerator_application/#10-business-model-revenue","title":"10. Business model &amp; revenue","text":"<ol> <li>Cloud SaaS: usage-based price per active workflow.</li> <li>Enterprise licence: on-prem, RBAC, audit, SLA support.</li> <li>Marketplace rev-share: paid third-party skills.</li> </ol>"},{"location":"accelerator_application/#11-key-metrics","title":"11. Key metrics","text":"Metric Why it matters Measurement Weekly Active Workflows Core engagement Internal analytics Avg. Cost-per-Run Demonstrates ROI Engine telemetry Retention of paying workspaces Product-market fit Billing data OSS community size Top-of-funnel GitHub stars / Slack members"},{"location":"accelerator_application/#12-market-opportunity","title":"12. Market opportunity","text":"<p>IDC projects AI-powered automation at $30 B TAM by 2028.  Capturing 1 % share yields a $300 M annual revenue path.</p>"},{"location":"accelerator_application/#13-traction","title":"13. Traction","text":"<ul> <li>1.2 k GitHub stars, 140 Slack members in six weeks.</li> <li>Infinite canvas in private-alpha with 3 design-partner companies.</li> <li>Wait-list: 220 sign-ups, growing 25 % WoW.</li> </ul>"},{"location":"accelerator_application/#14-scale-gtm","title":"14. Scale &amp; GTM","text":"<p>Land-and-expand OSS \u2192 self-serve cloud \u2192 enterprise upsell. - Content &amp; DevRel to acquire users (CAC \u2248 $350 projected). - Marketplace recommendations drive expansion inside accounts.</p>"},{"location":"accelerator_application/#15-ip-unique-advantages","title":"15. IP / unique advantages","text":"<ul> <li>Trademark on iceOS and Frosty Copilot.</li> <li>Trade-secret optimisation heuristics (self-learning runtime).</li> <li>Preparing provisional patent on self-optimising AI workflow execution.</li> </ul>"},{"location":"accelerator_application/#16-sustainable-competitive-advantage","title":"16. Sustainable competitive advantage","text":"<ul> <li>Data network effect: optimiser improves with every run across installs.</li> <li>Ecosystem lock-in: bespoke skills &amp; saved canvases raise switching costs.</li> <li>Enterprise on-prem option difficult for SaaS-only rivals to match.</li> </ul>"},{"location":"accelerator_application/#17-key-hires-needed","title":"17. Key hires needed","text":"<ol> <li>Senior front-end engineer (React-Flow, real-time collab)</li> <li>Applied ML engineer (prompt-to-workflow reasoning)</li> <li>DevRel lead (education &amp; community)</li> </ol>"},{"location":"accelerator_application/#18-capital-raised-to-date","title":"18. Capital raised to date","text":"<p>$0 external (bootstrapped with &lt;$30 k sweat equity + $25 k AWS credits).</p>"},{"location":"accelerator_application/#19-sources-of-capital","title":"19. Sources of capital","text":"<p>Founders' personal funds and AWS Activate.</p>"},{"location":"accelerator_application/#20-attachments","title":"20. Attachments","text":"<ul> <li>Pitch deck: https://tinyurl.com/iceOS-deck</li> <li>2-min demo video: https://tinyurl.com/iceOS-demo-video</li> </ul> <p>(Replace links with final assets before submission.) </p>"},{"location":"api_reference/","title":"API Reference","text":"<p>The sections below are auto-generated via <code>mkdocstrings</code>.  Use the search bar for quick lookup.</p>"},{"location":"api_reference/#ice_sdk_1","title":"ice_sdk","text":"<p>iceOS SDK.</p>"},{"location":"api_reference/#ice_sdk.BaseNode","title":"<code>BaseNode</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Abstract base class for all nodes.</p> <p>Provides: - Lifecycle hooks (pre_execute, post_execute) - Input validation using schema - Core node properties and configuration</p> Source code in <code>src/ice_sdk/base_node.py</code> <pre><code>class BaseNode(ABC):\n    \"\"\"Abstract base class for all nodes.\n\n    Provides:\n    - Lifecycle hooks (pre_execute, post_execute)\n    - Input validation using schema\n    - Core node properties and configuration\n    \"\"\"\n\n    def __init__(self, config: NodeConfig):\n        self.config = config\n\n    # ------------------------------------------------------------------\n    # Convenience accessors\n    # ------------------------------------------------------------------\n    @property\n    def node_id(self) -&gt; str:  # noqa: D401\n        return self.config.metadata.node_id  # type: ignore[attr-defined]\n\n    @property\n    def node_type(self) -&gt; str:  # noqa: D401\n        return self.config.metadata.node_type  # type: ignore[attr-defined]\n\n    @property\n    def id(self):  # noqa: D401\n        return self.config.id\n\n    @property\n    def llm_config(self):  # noqa: D401\n        return getattr(self, \"_llm_config\", getattr(self.config, \"llm_config\", None))\n\n    @property\n    def dependencies(self):  # noqa: D401\n        return self.config.dependencies\n\n    # ------------------------------------------------------------------\n    # Lifecycle hooks ----------------------------------------------------\n    # ------------------------------------------------------------------\n    async def pre_execute(self, context: Dict[str, Any]):  # noqa: D401\n        \"\"\"Validate and potentially transform *context* before :py:meth:`execute`.\"\"\"\n        if not await self.validate_input(context):\n            raise ValueError(\"Input validation failed\")\n        return context\n\n    async def post_execute(self, result: NodeExecutionResult):  # noqa: D401\n        return result\n\n    # ------------------------------------------------------------------\n    # Validation helpers -------------------------------------------------\n    # ------------------------------------------------------------------\n    async def validate_input(self, context: Dict[str, Any]) -&gt; bool:  # noqa: D401\n        # Allow dynamic schema adaptation based on context.\n        if hasattr(self.config, \"adapt_schema_from_context\"):\n            self.config.adapt_schema_from_context(context)  # type: ignore[attr-defined]\n\n        schema = self.config.input_schema\n        if not schema:\n            return True\n\n        if hasattr(\n            self.config, \"is_pydantic_schema\"\n        ) and self.config.is_pydantic_schema(schema):\n            try:\n                schema.model_validate(context)  # type: ignore[attr-defined]\n                return True\n            except ValidationError:\n                return False\n\n        # Dict-based validation fallback.\n        try:\n            fields = {\n                key: (eval(type_str), ...) for key, type_str in schema.items()\n            }  # noqa: S307 \u2013 eval on trusted input\n            InputModel = create_model(\"InputModel\", **fields)  # type: ignore[call-arg,call-overload]\n            InputModel(**context)  # type: ignore[call-arg]\n            return True\n        except (ValidationError, NameError, SyntaxError):\n            return False\n\n    # ------------------------------------------------------------------\n    # Abstract method ----------------------------------------------------\n    # ------------------------------------------------------------------\n    @abstractmethod\n    async def execute(\n        self, context: Dict[str, Any]\n    ) -&gt; NodeExecutionResult:  # noqa: D401\n        \"\"\"Execute node logic (to be provided by subclasses).\"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"api_reference/#ice_sdk.BaseNode.execute","title":"<code>execute(context)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Execute node logic (to be provided by subclasses).</p> Source code in <code>src/ice_sdk/base_node.py</code> <pre><code>@abstractmethod\nasync def execute(\n    self, context: Dict[str, Any]\n) -&gt; NodeExecutionResult:  # noqa: D401\n    \"\"\"Execute node logic (to be provided by subclasses).\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api_reference/#ice_sdk.BaseNode.pre_execute","title":"<code>pre_execute(context)</code>  <code>async</code>","text":"<p>Validate and potentially transform context before :py:meth:<code>execute</code>.</p> Source code in <code>src/ice_sdk/base_node.py</code> <pre><code>async def pre_execute(self, context: Dict[str, Any]):  # noqa: D401\n    \"\"\"Validate and potentially transform *context* before :py:meth:`execute`.\"\"\"\n    if not await self.validate_input(context):\n        raise ValueError(\"Input validation failed\")\n    return context\n</code></pre>"},{"location":"api_reference/#ice_sdk.BaseTool","title":"<code>BaseTool</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Base class for all tools.</p> <p><code>name</code>/<code>description</code>/<code>parameters_schema</code>/<code>output_schema</code> are defined as class variables so subclasses can simply override them without Pydantic treating them as model fields (fixes the field-override error we hit during testing).</p> Source code in <code>src/ice_sdk/tools/base.py</code> <pre><code>class BaseTool(BaseModel):\n    \"\"\"Base class for all tools.\n\n    ``name``/``description``/``parameters_schema``/``output_schema`` are defined\n    as *class variables* so subclasses can simply override them without Pydantic\n    treating them as model fields (fixes the field-override error we hit during\n    testing).\n    \"\"\"\n\n    # Tool metadata ----------------------------------------------------------------\n    name: ClassVar[str] = \"\"\n    description: ClassVar[str] = \"\"\n    parameters_schema: ClassVar[Dict[str, Any] | None] = None\n    output_schema: ClassVar[Dict[str, Any] | None] = None\n\n    async def run(self, **kwargs) -&gt; Any:\n        \"\"\"Execute the tool with the given arguments.\"\"\"\n        raise NotImplementedError\n\n    def as_dict(self) -&gt; Dict[str, Any]:\n        \"\"\"Convert tool to dictionary format.\"\"\"\n        return {\n            \"name\": self.name,\n            \"description\": self.description,\n            \"parameters\": self.parameters_schema,\n            \"output\": self.output_schema,\n        }\n</code></pre>"},{"location":"api_reference/#ice_sdk.BaseTool.as_dict","title":"<code>as_dict()</code>","text":"<p>Convert tool to dictionary format.</p> Source code in <code>src/ice_sdk/tools/base.py</code> <pre><code>def as_dict(self) -&gt; Dict[str, Any]:\n    \"\"\"Convert tool to dictionary format.\"\"\"\n    return {\n        \"name\": self.name,\n        \"description\": self.description,\n        \"parameters\": self.parameters_schema,\n        \"output\": self.output_schema,\n    }\n</code></pre>"},{"location":"api_reference/#ice_sdk.BaseTool.run","title":"<code>run(**kwargs)</code>  <code>async</code>","text":"<p>Execute the tool with the given arguments.</p> Source code in <code>src/ice_sdk/tools/base.py</code> <pre><code>async def run(self, **kwargs) -&gt; Any:\n    \"\"\"Execute the tool with the given arguments.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api_reference/#ice_sdk.GraphContextManager","title":"<code>GraphContextManager</code>","text":"<p>Orchestrates context management for graph-based LLM node execution. Handles tool execution, context flow, and agent coordination.</p> Source code in <code>src/ice_sdk/context/manager.py</code> <pre><code>class GraphContextManager:\n    \"\"\"\n    Orchestrates context management for graph-based LLM node execution.\n    Handles tool execution, context flow, and agent coordination.\n    \"\"\"\n\n    def __init__(\n        self,\n        max_tokens: int = 4000,\n        *,\n        max_sessions: int = 10,\n        graph: Optional[nx.DiGraph] = None,\n        store: Optional[ContextStore] = None,\n        formatter: Optional[ContextFormatter] = None,\n    ):\n        \"\"\"Create a ``GraphContextManager``.\n\n        Args:\n            max_tokens: Soft token window enforced when persisting context.\n            max_sessions: Number of distinct *session_id*s to keep in memory\n                before evicting the least-recently-used.  Old sessions can still\n                be re-created on demand but any cached context is dropped.\n        \"\"\"\n        from collections import OrderedDict\n\n        self.max_tokens = max_tokens\n        self.max_sessions = max_sessions\n        self.graph = graph or nx.DiGraph()\n        self.store = store or ContextStore()\n        self.formatter = formatter or ContextFormatter()\n        self._agents: Dict[str, 'AgentNode'] = {}\n        self._tools: Dict[str, BaseTool] = {}\n        # Map of session_id -&gt; GraphContext (acts as LRU cache) --------------\n        self._contexts: 'OrderedDict[str, GraphContext]' = OrderedDict()\n        self._context: Optional[GraphContext] = None\n\n    def register_agent(self, agent: 'AgentNode') -&gt; None:\n        \"\"\"Register an agent for lookup by other agents.\"\"\"\n        if agent.config.name in self._agents:\n            raise ValueError(f\"Agent '{agent.config.name}' already registered\")\n        self._agents[agent.config.name] = agent\n\n    def register_tool(self, tool: BaseTool) -&gt; None:\n        \"\"\"Register a tool for use by agents.\n\n        Args:\n            tool: Tool to register\n        \"\"\"\n        if tool.name in self._tools:\n            raise ValueError(f\"Tool '{tool.name}' already registered\")\n        self._tools[tool.name] = tool\n\n    def get_agent(self, name: str) -&gt; Optional['AgentNode']:\n        \"\"\"Look up an agent by name.\"\"\"\n        return self._agents.get(name)\n\n    def get_tool(self, name: str) -&gt; Optional[BaseTool]:\n        \"\"\"Get registered tool by name.\"\"\"\n        return self._tools.get(name)\n\n    def get_all_agents(self) -&gt; Dict[str, 'AgentNode']:\n        \"\"\"Get all registered agents.\"\"\"\n        return dict(self._agents)\n\n    def get_all_tools(self) -&gt; Dict[str, BaseTool]:\n        \"\"\"Get all registered tools.\"\"\"\n        return dict(self._tools)\n\n    def get_context(self, session_id: Optional[str] = None) -&gt; Optional[GraphContext]:\n        \"\"\"Return the current :class:`GraphContext`.\n\n        When *session_id* is provided, the manager ensures that the returned\n        context matches that ID\u2014creating a **new** context if necessary.  This\n        prevents different chains sharing a manager from leaking data into one\n        another.\n        \"\"\"\n        # No special handling requested \u2013 maintain legacy behaviour ----------\n        if session_id is None:\n            return self._context\n\n        # No context yet or session mismatch \u2192 start/rotate ------------------\n        if self._context is None or self._context.session_id != session_id:\n            self._context = GraphContext(session_id=session_id)\n            self._register_context(self._context)\n\n        return self._context\n\n    def set_context(self, context: GraphContext) -&gt; None:\n        \"\"\"Set execution context.\"\"\"\n        self._context = context\n        self._register_context(context)\n\n    async def execute_tool(\n        self,\n        tool_name: str,\n        **kwargs\n    ) -&gt; Any:\n        \"\"\"Execute a tool with the current context.\n\n        Args:\n            tool_name: Name of tool to execute\n            **kwargs: Tool arguments\n        \"\"\"\n        tool = self.get_tool(tool_name)\n        if not tool:\n            raise ValueError(f\"Tool '{tool_name}' not found\")\n\n        if not self._context:\n            raise RuntimeError(\"No execution context set\")\n\n        # Create tool context\n        tool_ctx = ToolContext(\n            agent_id=self._context.session_id,\n            session_id=self._context.execution_id or self._context.session_id,\n            metadata=self._context.metadata\n        )\n\n        # Execute tool\n        try:\n            result = await tool.run(ctx=tool_ctx, **kwargs)\n            return result\n        except Exception as e:\n            logger.error(f\"Tool execution failed: {str(e)}\")\n            raise\n\n    def update_node_context(\n        self,\n        node_id: str,\n        content: Any,\n        execution_id: Optional[str] = None,\n        schema: Optional[Dict[str, str]] = None,\n    ) -&gt; None:\n        \"\"\"Update context for a specific node.\"\"\"\n        # ------------------------------------------------------------------\n        # Enforce *max_tokens* window per GraphContextManager configuration --\n        # ------------------------------------------------------------------\n        try:\n            # Serialize *content* for counting/truncation.  Non-string payloads\n            # are converted to JSON-ish string so the token approximation is\n            # still meaningful.\n            import json\n\n            from ice_sdk.models.config import ModelProvider\n            from ice_sdk.utils.token_counter import TokenCounter\n\n            if isinstance(content, str):\n                serialised = content\n            else:\n                try:\n                    serialised = json.dumps(content, ensure_ascii=False, default=str)\n                except TypeError:\n                    serialised = str(content)\n\n            current_tokens = TokenCounter.estimate_tokens(serialised, model=\"\", provider=ModelProvider.CUSTOM)\n\n            if self.max_tokens and current_tokens &gt; self.max_tokens:\n                # Truncate string representation to fit token budget (\u22484 chars/token)\n                char_budget = self.max_tokens * 4\n                serialised = serialised[:char_budget]\n\n                # Try to re-parse back to original type when possible ---------\n                try:\n                    truncated_content = json.loads(serialised)\n                except Exception:\n                    truncated_content = serialised\n                content = truncated_content\n        except Exception:  # pragma: no cover \u2013 fallback when tiktoken missing\n            # On failure, fall back to char-length based heuristic.\n            if self.max_tokens and isinstance(content, str):\n                char_budget = self.max_tokens * 4\n                if len(content) &gt; char_budget:\n                    content = content[:char_budget]\n\n        # Persist via underlying store --------------------------------------\n        self.store.update(node_id, content, execution_id=execution_id, schema=schema)\n\n    def get_node_context(self, node_id: str) -&gt; Any:\n        \"\"\"Get context for a specific node.\"\"\"\n        return self.store.get(node_id)\n\n    def clear_node_context(self, node_id: Optional[str] = None) -&gt; None:\n        \"\"\"Clear context for a specific node or all nodes.\"\"\"\n        self.store.clear(node_id)\n\n    def format_context(\n        self,\n        content: Any,\n        rule: str,\n        format_specs: Optional[Dict[str, Any]] = None\n    ) -&gt; str:\n        \"\"\"Format context content according to rules.\"\"\"\n        return self.formatter.format(content, rule, format_specs)\n\n    def validate_context_rules(self, rules: Dict[str, Any]) -&gt; bool:\n        \"\"\"Validate context rules against graph structure.\"\"\"\n        for node_id, rule in rules.items():\n            if node_id not in self.graph.nodes:\n                logger.warning(f\"Context rule specified for non-existent node: {node_id}\")\n                return False\n            if (\n                hasattr(rule, \"max_tokens\")\n                and rule.max_tokens\n                and rule.max_tokens &gt; self.max_tokens\n            ):\n                logger.warning(f\"Context rule max_tokens exceeds system limit for node: {node_id}\")\n                return False\n        return True\n\n    def log_error(self, node_id: str, error: Exception) -&gt; None:\n        logger.error(f\"Error in node {node_id}: {str(error)}\")\n\n    def list_agents(self) -&gt; List[str]:\n        \"\"\"List registered agent names.\"\"\"\n        return list(self._agents.keys())\n\n    def list_tools(self) -&gt; List[str]:\n        \"\"\"List registered tool names.\"\"\"\n        return list(self._tools.keys())\n\n    # ------------------------------------------------------------------\n    # Experimental helpers ---------------------------------------------\n    # ------------------------------------------------------------------\n\n    def smart_context_compression(\n        self,\n        content: Any,\n        *,\n        schema: Optional[Dict[str, Any]] = None,\n        strategy: str = \"summarize\",\n        max_tokens: Optional[int] = None,\n    ) -&gt; Any:\n        \"\"\"Return a compressed variant of *content* according to *strategy*.\n\n        This helper is **best-effort** and deliberately side-effect-free; it is\n        safe to call within hot code-paths (e.g. during context updates).\n\n        Parameters\n        ----------\n        content\n            Arbitrary serialisable payload to compress.\n        schema\n            Optional type/schema information that can guide the compression\n            algorithm (e.g. know which keys are essential).\n        strategy\n            Supported values:\n            ``\"summarize\"``  \u2013 Short text summary (default)\n            ``\"truncate\"``   \u2013 Hard trim to *max_tokens* (or manager-level limit)\n            ``\"embed\"``      \u2013 Placeholder for embedding-based selection\n        max_tokens\n            Optional override for the *GraphContextManager.max_tokens* limit.\n        \"\"\"\n\n        # Fallback defaults --------------------------------------------\n        effective_max_tokens = max_tokens or self.max_tokens\n\n        # Import token counter lazily to avoid heavy startup costs\n        from ice_sdk.models.config import ModelProvider\n        from ice_sdk.utils.token_counter import TokenCounter\n\n        def _estimate_tokens(text: str) -&gt; int:\n            return TokenCounter.estimate_tokens(text, model=\"\", provider=ModelProvider.CUSTOM)\n\n        # ------------------------------------------------------------------\n        # Strategy: truncate (cheap) ---------------------------------------\n        # ------------------------------------------------------------------\n        if strategy == \"truncate\":\n            if isinstance(content, str):\n                tokens = _estimate_tokens(content)\n                if effective_max_tokens and tokens &gt; effective_max_tokens:\n                    char_budget = effective_max_tokens * 4  # \u22484 chars/token\n                    return content[:char_budget]\n            return content  # nothing to do or non-str payload\n\n        # ------------------------------------------------------------------\n        # Strategy: summarize (LLM heavy) ----------------------------------\n        # ------------------------------------------------------------------\n        if strategy == \"summarize\":\n            try:\n                # Defer import \u2013 summariser is optional dependency\n                from ice_sdk.tools.builtins.deterministic import (\n                    deterministic_summariser,  # type: ignore\n                )\n\n                summary = deterministic_summariser(content, schema=schema, max_tokens=effective_max_tokens)\n                return summary\n            except Exception:  # noqa: BLE001\n                # Fall back to plain truncation when summarisation unavailable\n                return self.smart_context_compression(\n                    content,\n                    schema=schema,\n                    strategy=\"truncate\",\n                    max_tokens=effective_max_tokens,\n                )\n\n        # ------------------------------------------------------------------\n        # Strategy: embed (placeholder \u2013 to be implemented) ----------------\n        # ------------------------------------------------------------------\n        if strategy == \"embed\":\n            # Placeholder \u2013 will be implemented when embedding service lands\n            return self.smart_context_compression(\n                content,\n                schema=schema,\n                strategy=\"truncate\",\n                max_tokens=effective_max_tokens,\n            )\n\n        # Unknown strategy \u2192 pass through unchanged ------------------------\n        return content\n\n    # ------------------------------------------------------------------\n    # Internal helpers ---------------------------------------------------\n    # ------------------------------------------------------------------\n    def _register_context(self, ctx: GraphContext) -&gt; None:\n        \"\"\"Insert *ctx* into the LRU map and evict when necessary.\"\"\"\n        # Pop existing entry so we can push it to the right end (most recent)\n        self._contexts.pop(ctx.session_id, None)\n        self._contexts[ctx.session_id] = ctx\n\n        # Evict oldest sessions above limit --------------------------------\n        while len(self._contexts) &gt; self.max_sessions:\n            old_sess, _ = self._contexts.popitem(last=False)\n            # Purge persisted node-context data for the evicted session -----\n            # We assume node_ids are prefixed with session_id or otherwise\n            # unique; if not, users can still call ContextStore.clear().\n            # Here we only drop the in-memory reference.\n            if self._context and self._context.session_id == old_sess:\n                self._context = None\n</code></pre>"},{"location":"api_reference/#ice_sdk.GraphContextManager.__init__","title":"<code>__init__(max_tokens=4000, *, max_sessions=10, graph=None, store=None, formatter=None)</code>","text":"<p>Create a <code>GraphContextManager</code>.</p> <p>Parameters:</p> Name Type Description Default <code>max_tokens</code> <code>int</code> <p>Soft token window enforced when persisting context.</p> <code>4000</code> <code>max_sessions</code> <code>int</code> <p>Number of distinct session_ids to keep in memory before evicting the least-recently-used.  Old sessions can still be re-created on demand but any cached context is dropped.</p> <code>10</code> Source code in <code>src/ice_sdk/context/manager.py</code> <pre><code>def __init__(\n    self,\n    max_tokens: int = 4000,\n    *,\n    max_sessions: int = 10,\n    graph: Optional[nx.DiGraph] = None,\n    store: Optional[ContextStore] = None,\n    formatter: Optional[ContextFormatter] = None,\n):\n    \"\"\"Create a ``GraphContextManager``.\n\n    Args:\n        max_tokens: Soft token window enforced when persisting context.\n        max_sessions: Number of distinct *session_id*s to keep in memory\n            before evicting the least-recently-used.  Old sessions can still\n            be re-created on demand but any cached context is dropped.\n    \"\"\"\n    from collections import OrderedDict\n\n    self.max_tokens = max_tokens\n    self.max_sessions = max_sessions\n    self.graph = graph or nx.DiGraph()\n    self.store = store or ContextStore()\n    self.formatter = formatter or ContextFormatter()\n    self._agents: Dict[str, 'AgentNode'] = {}\n    self._tools: Dict[str, BaseTool] = {}\n    # Map of session_id -&gt; GraphContext (acts as LRU cache) --------------\n    self._contexts: 'OrderedDict[str, GraphContext]' = OrderedDict()\n    self._context: Optional[GraphContext] = None\n</code></pre>"},{"location":"api_reference/#ice_sdk.GraphContextManager.clear_node_context","title":"<code>clear_node_context(node_id=None)</code>","text":"<p>Clear context for a specific node or all nodes.</p> Source code in <code>src/ice_sdk/context/manager.py</code> <pre><code>def clear_node_context(self, node_id: Optional[str] = None) -&gt; None:\n    \"\"\"Clear context for a specific node or all nodes.\"\"\"\n    self.store.clear(node_id)\n</code></pre>"},{"location":"api_reference/#ice_sdk.GraphContextManager.execute_tool","title":"<code>execute_tool(tool_name, **kwargs)</code>  <code>async</code>","text":"<p>Execute a tool with the current context.</p> <p>Parameters:</p> Name Type Description Default <code>tool_name</code> <code>str</code> <p>Name of tool to execute</p> required <code>**kwargs</code> <p>Tool arguments</p> <code>{}</code> Source code in <code>src/ice_sdk/context/manager.py</code> <pre><code>async def execute_tool(\n    self,\n    tool_name: str,\n    **kwargs\n) -&gt; Any:\n    \"\"\"Execute a tool with the current context.\n\n    Args:\n        tool_name: Name of tool to execute\n        **kwargs: Tool arguments\n    \"\"\"\n    tool = self.get_tool(tool_name)\n    if not tool:\n        raise ValueError(f\"Tool '{tool_name}' not found\")\n\n    if not self._context:\n        raise RuntimeError(\"No execution context set\")\n\n    # Create tool context\n    tool_ctx = ToolContext(\n        agent_id=self._context.session_id,\n        session_id=self._context.execution_id or self._context.session_id,\n        metadata=self._context.metadata\n    )\n\n    # Execute tool\n    try:\n        result = await tool.run(ctx=tool_ctx, **kwargs)\n        return result\n    except Exception as e:\n        logger.error(f\"Tool execution failed: {str(e)}\")\n        raise\n</code></pre>"},{"location":"api_reference/#ice_sdk.GraphContextManager.format_context","title":"<code>format_context(content, rule, format_specs=None)</code>","text":"<p>Format context content according to rules.</p> Source code in <code>src/ice_sdk/context/manager.py</code> <pre><code>def format_context(\n    self,\n    content: Any,\n    rule: str,\n    format_specs: Optional[Dict[str, Any]] = None\n) -&gt; str:\n    \"\"\"Format context content according to rules.\"\"\"\n    return self.formatter.format(content, rule, format_specs)\n</code></pre>"},{"location":"api_reference/#ice_sdk.GraphContextManager.get_agent","title":"<code>get_agent(name)</code>","text":"<p>Look up an agent by name.</p> Source code in <code>src/ice_sdk/context/manager.py</code> <pre><code>def get_agent(self, name: str) -&gt; Optional['AgentNode']:\n    \"\"\"Look up an agent by name.\"\"\"\n    return self._agents.get(name)\n</code></pre>"},{"location":"api_reference/#ice_sdk.GraphContextManager.get_all_agents","title":"<code>get_all_agents()</code>","text":"<p>Get all registered agents.</p> Source code in <code>src/ice_sdk/context/manager.py</code> <pre><code>def get_all_agents(self) -&gt; Dict[str, 'AgentNode']:\n    \"\"\"Get all registered agents.\"\"\"\n    return dict(self._agents)\n</code></pre>"},{"location":"api_reference/#ice_sdk.GraphContextManager.get_all_tools","title":"<code>get_all_tools()</code>","text":"<p>Get all registered tools.</p> Source code in <code>src/ice_sdk/context/manager.py</code> <pre><code>def get_all_tools(self) -&gt; Dict[str, BaseTool]:\n    \"\"\"Get all registered tools.\"\"\"\n    return dict(self._tools)\n</code></pre>"},{"location":"api_reference/#ice_sdk.GraphContextManager.get_context","title":"<code>get_context(session_id=None)</code>","text":"<p>Return the current :class:<code>GraphContext</code>.</p> <p>When session_id is provided, the manager ensures that the returned context matches that ID\u2014creating a new context if necessary.  This prevents different chains sharing a manager from leaking data into one another.</p> Source code in <code>src/ice_sdk/context/manager.py</code> <pre><code>def get_context(self, session_id: Optional[str] = None) -&gt; Optional[GraphContext]:\n    \"\"\"Return the current :class:`GraphContext`.\n\n    When *session_id* is provided, the manager ensures that the returned\n    context matches that ID\u2014creating a **new** context if necessary.  This\n    prevents different chains sharing a manager from leaking data into one\n    another.\n    \"\"\"\n    # No special handling requested \u2013 maintain legacy behaviour ----------\n    if session_id is None:\n        return self._context\n\n    # No context yet or session mismatch \u2192 start/rotate ------------------\n    if self._context is None or self._context.session_id != session_id:\n        self._context = GraphContext(session_id=session_id)\n        self._register_context(self._context)\n\n    return self._context\n</code></pre>"},{"location":"api_reference/#ice_sdk.GraphContextManager.get_node_context","title":"<code>get_node_context(node_id)</code>","text":"<p>Get context for a specific node.</p> Source code in <code>src/ice_sdk/context/manager.py</code> <pre><code>def get_node_context(self, node_id: str) -&gt; Any:\n    \"\"\"Get context for a specific node.\"\"\"\n    return self.store.get(node_id)\n</code></pre>"},{"location":"api_reference/#ice_sdk.GraphContextManager.get_tool","title":"<code>get_tool(name)</code>","text":"<p>Get registered tool by name.</p> Source code in <code>src/ice_sdk/context/manager.py</code> <pre><code>def get_tool(self, name: str) -&gt; Optional[BaseTool]:\n    \"\"\"Get registered tool by name.\"\"\"\n    return self._tools.get(name)\n</code></pre>"},{"location":"api_reference/#ice_sdk.GraphContextManager.list_agents","title":"<code>list_agents()</code>","text":"<p>List registered agent names.</p> Source code in <code>src/ice_sdk/context/manager.py</code> <pre><code>def list_agents(self) -&gt; List[str]:\n    \"\"\"List registered agent names.\"\"\"\n    return list(self._agents.keys())\n</code></pre>"},{"location":"api_reference/#ice_sdk.GraphContextManager.list_tools","title":"<code>list_tools()</code>","text":"<p>List registered tool names.</p> Source code in <code>src/ice_sdk/context/manager.py</code> <pre><code>def list_tools(self) -&gt; List[str]:\n    \"\"\"List registered tool names.\"\"\"\n    return list(self._tools.keys())\n</code></pre>"},{"location":"api_reference/#ice_sdk.GraphContextManager.register_agent","title":"<code>register_agent(agent)</code>","text":"<p>Register an agent for lookup by other agents.</p> Source code in <code>src/ice_sdk/context/manager.py</code> <pre><code>def register_agent(self, agent: 'AgentNode') -&gt; None:\n    \"\"\"Register an agent for lookup by other agents.\"\"\"\n    if agent.config.name in self._agents:\n        raise ValueError(f\"Agent '{agent.config.name}' already registered\")\n    self._agents[agent.config.name] = agent\n</code></pre>"},{"location":"api_reference/#ice_sdk.GraphContextManager.register_tool","title":"<code>register_tool(tool)</code>","text":"<p>Register a tool for use by agents.</p> <p>Parameters:</p> Name Type Description Default <code>tool</code> <code>BaseTool</code> <p>Tool to register</p> required Source code in <code>src/ice_sdk/context/manager.py</code> <pre><code>def register_tool(self, tool: BaseTool) -&gt; None:\n    \"\"\"Register a tool for use by agents.\n\n    Args:\n        tool: Tool to register\n    \"\"\"\n    if tool.name in self._tools:\n        raise ValueError(f\"Tool '{tool.name}' already registered\")\n    self._tools[tool.name] = tool\n</code></pre>"},{"location":"api_reference/#ice_sdk.GraphContextManager.set_context","title":"<code>set_context(context)</code>","text":"<p>Set execution context.</p> Source code in <code>src/ice_sdk/context/manager.py</code> <pre><code>def set_context(self, context: GraphContext) -&gt; None:\n    \"\"\"Set execution context.\"\"\"\n    self._context = context\n    self._register_context(context)\n</code></pre>"},{"location":"api_reference/#ice_sdk.GraphContextManager.smart_context_compression","title":"<code>smart_context_compression(content, *, schema=None, strategy='summarize', max_tokens=None)</code>","text":"<p>Return a compressed variant of content according to strategy.</p> <p>This helper is best-effort and deliberately side-effect-free; it is safe to call within hot code-paths (e.g. during context updates).</p>"},{"location":"api_reference/#ice_sdk.GraphContextManager.smart_context_compression--parameters","title":"Parameters","text":"<p>content     Arbitrary serialisable payload to compress. schema     Optional type/schema information that can guide the compression     algorithm (e.g. know which keys are essential). strategy     Supported values:     <code>\"summarize\"</code>  \u2013 Short text summary (default)     <code>\"truncate\"</code>   \u2013 Hard trim to max_tokens (or manager-level limit)     <code>\"embed\"</code>      \u2013 Placeholder for embedding-based selection max_tokens     Optional override for the GraphContextManager.max_tokens limit.</p> Source code in <code>src/ice_sdk/context/manager.py</code> <pre><code>def smart_context_compression(\n    self,\n    content: Any,\n    *,\n    schema: Optional[Dict[str, Any]] = None,\n    strategy: str = \"summarize\",\n    max_tokens: Optional[int] = None,\n) -&gt; Any:\n    \"\"\"Return a compressed variant of *content* according to *strategy*.\n\n    This helper is **best-effort** and deliberately side-effect-free; it is\n    safe to call within hot code-paths (e.g. during context updates).\n\n    Parameters\n    ----------\n    content\n        Arbitrary serialisable payload to compress.\n    schema\n        Optional type/schema information that can guide the compression\n        algorithm (e.g. know which keys are essential).\n    strategy\n        Supported values:\n        ``\"summarize\"``  \u2013 Short text summary (default)\n        ``\"truncate\"``   \u2013 Hard trim to *max_tokens* (or manager-level limit)\n        ``\"embed\"``      \u2013 Placeholder for embedding-based selection\n    max_tokens\n        Optional override for the *GraphContextManager.max_tokens* limit.\n    \"\"\"\n\n    # Fallback defaults --------------------------------------------\n    effective_max_tokens = max_tokens or self.max_tokens\n\n    # Import token counter lazily to avoid heavy startup costs\n    from ice_sdk.models.config import ModelProvider\n    from ice_sdk.utils.token_counter import TokenCounter\n\n    def _estimate_tokens(text: str) -&gt; int:\n        return TokenCounter.estimate_tokens(text, model=\"\", provider=ModelProvider.CUSTOM)\n\n    # ------------------------------------------------------------------\n    # Strategy: truncate (cheap) ---------------------------------------\n    # ------------------------------------------------------------------\n    if strategy == \"truncate\":\n        if isinstance(content, str):\n            tokens = _estimate_tokens(content)\n            if effective_max_tokens and tokens &gt; effective_max_tokens:\n                char_budget = effective_max_tokens * 4  # \u22484 chars/token\n                return content[:char_budget]\n        return content  # nothing to do or non-str payload\n\n    # ------------------------------------------------------------------\n    # Strategy: summarize (LLM heavy) ----------------------------------\n    # ------------------------------------------------------------------\n    if strategy == \"summarize\":\n        try:\n            # Defer import \u2013 summariser is optional dependency\n            from ice_sdk.tools.builtins.deterministic import (\n                deterministic_summariser,  # type: ignore\n            )\n\n            summary = deterministic_summariser(content, schema=schema, max_tokens=effective_max_tokens)\n            return summary\n        except Exception:  # noqa: BLE001\n            # Fall back to plain truncation when summarisation unavailable\n            return self.smart_context_compression(\n                content,\n                schema=schema,\n                strategy=\"truncate\",\n                max_tokens=effective_max_tokens,\n            )\n\n    # ------------------------------------------------------------------\n    # Strategy: embed (placeholder \u2013 to be implemented) ----------------\n    # ------------------------------------------------------------------\n    if strategy == \"embed\":\n        # Placeholder \u2013 will be implemented when embedding service lands\n        return self.smart_context_compression(\n            content,\n            schema=schema,\n            strategy=\"truncate\",\n            max_tokens=effective_max_tokens,\n        )\n\n    # Unknown strategy \u2192 pass through unchanged ------------------------\n    return content\n</code></pre>"},{"location":"api_reference/#ice_sdk.GraphContextManager.update_node_context","title":"<code>update_node_context(node_id, content, execution_id=None, schema=None)</code>","text":"<p>Update context for a specific node.</p> Source code in <code>src/ice_sdk/context/manager.py</code> <pre><code>def update_node_context(\n    self,\n    node_id: str,\n    content: Any,\n    execution_id: Optional[str] = None,\n    schema: Optional[Dict[str, str]] = None,\n) -&gt; None:\n    \"\"\"Update context for a specific node.\"\"\"\n    # ------------------------------------------------------------------\n    # Enforce *max_tokens* window per GraphContextManager configuration --\n    # ------------------------------------------------------------------\n    try:\n        # Serialize *content* for counting/truncation.  Non-string payloads\n        # are converted to JSON-ish string so the token approximation is\n        # still meaningful.\n        import json\n\n        from ice_sdk.models.config import ModelProvider\n        from ice_sdk.utils.token_counter import TokenCounter\n\n        if isinstance(content, str):\n            serialised = content\n        else:\n            try:\n                serialised = json.dumps(content, ensure_ascii=False, default=str)\n            except TypeError:\n                serialised = str(content)\n\n        current_tokens = TokenCounter.estimate_tokens(serialised, model=\"\", provider=ModelProvider.CUSTOM)\n\n        if self.max_tokens and current_tokens &gt; self.max_tokens:\n            # Truncate string representation to fit token budget (\u22484 chars/token)\n            char_budget = self.max_tokens * 4\n            serialised = serialised[:char_budget]\n\n            # Try to re-parse back to original type when possible ---------\n            try:\n                truncated_content = json.loads(serialised)\n            except Exception:\n                truncated_content = serialised\n            content = truncated_content\n    except Exception:  # pragma: no cover \u2013 fallback when tiktoken missing\n        # On failure, fall back to char-length based heuristic.\n        if self.max_tokens and isinstance(content, str):\n            char_budget = self.max_tokens * 4\n            if len(content) &gt; char_budget:\n                content = content[:char_budget]\n\n    # Persist via underlying store --------------------------------------\n    self.store.update(node_id, content, execution_id=execution_id, schema=schema)\n</code></pre>"},{"location":"api_reference/#ice_sdk.GraphContextManager.validate_context_rules","title":"<code>validate_context_rules(rules)</code>","text":"<p>Validate context rules against graph structure.</p> Source code in <code>src/ice_sdk/context/manager.py</code> <pre><code>def validate_context_rules(self, rules: Dict[str, Any]) -&gt; bool:\n    \"\"\"Validate context rules against graph structure.\"\"\"\n    for node_id, rule in rules.items():\n        if node_id not in self.graph.nodes:\n            logger.warning(f\"Context rule specified for non-existent node: {node_id}\")\n            return False\n        if (\n            hasattr(rule, \"max_tokens\")\n            and rule.max_tokens\n            and rule.max_tokens &gt; self.max_tokens\n        ):\n            logger.warning(f\"Context rule max_tokens exceeds system limit for node: {node_id}\")\n            return False\n    return True\n</code></pre>"},{"location":"api_reference/#ice_sdk.LLMConfig","title":"<code>LLMConfig</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Configuration for language models</p> Source code in <code>src/ice_sdk/models/config.py</code> <pre><code>class LLMConfig(BaseModel):\n    \"\"\"Configuration for language models\"\"\"\n\n    provider: Optional[str] = None\n    model: Optional[str] = None\n    temperature: Optional[float] = None\n    max_tokens: Optional[int] = None\n    max_context_tokens: Optional[int] = None\n    api_key: Optional[str] = None\n    top_p: Optional[float] = None\n    frequency_penalty: Optional[float] = None\n    presence_penalty: Optional[float] = None\n    stop_sequences: Optional[list[str]] = None\n    custom_parameters: Dict[str, Any] = Field(\n        default_factory=dict, description=\"Provider-specific parameters\"\n    )\n    model_config = ConfigDict(extra=\"allow\")\n\n    @field_validator(\"api_key\")\n    @classmethod\n    def validate_api_key(cls, v: Optional[str], info) -&gt; Optional[str]:\n        \"\"\"Validate API key format based on provider.\"\"\"\n        provider = info.data.get(\"provider\", ModelProvider.OPENAI)\n        if v is None:\n            # Allow missing api_key (will be loaded from env)\n            return v\n        if not v:  # All providers require an API key if provided\n            raise ValueError(f\"API key for provider {provider.value} cannot be empty.\")\n        # Allow test keys for any provider\n        if v.startswith(\"test-\"):\n            return v\n        if provider == ModelProvider.OPENAI:\n            if not (v.startswith(\"sk-\") and len(v) == 51) and not v.startswith(\n                \"sk-proj-\"\n            ):\n                logger.warning(\n                    \"OpenAI API key does not match standard formats (sk-..., sk-proj-...). Proceeding, but please verify.\"\n                )\n        elif provider == ModelProvider.ANTHROPIC:\n            if not v.startswith(\"sk-ant-\"):\n                logger.warning(\n                    \"Anthropic API key does not start with 'sk-ant-'. Proceeding, but please verify.\"\n                )\n        elif provider == ModelProvider.GOOGLE:\n            if len(v) &lt; 30:\n                logger.warning(\n                    \"Google (Gemini) API key seems short. Proceeding, but please verify.\"\n                )\n        elif provider == ModelProvider.DEEPSEEK:\n            if not v.startswith(\"sk-\"):\n                logger.warning(\n                    \"DeepSeek API key does not start with 'sk-'. Proceeding, but please verify its format.\"\n                )\n        elif provider == ModelProvider.CUSTOM:\n            pass\n        return v\n\n    @field_validator(\"model\")\n    @classmethod\n    def validate_model(cls, v: str, info) -&gt; str:\n        \"\"\"Validate model name based on provider.\"\"\"\n        provider = info.data.get(\"provider\", ModelProvider.OPENAI)\n        try:\n            parse_model_version(v, provider)\n            return v\n        except ValueError as e:\n            raise ValueError(f\"Invalid model for provider {provider}: {str(e)}\")\n</code></pre>"},{"location":"api_reference/#ice_sdk.LLMConfig.validate_api_key","title":"<code>validate_api_key(v, info)</code>  <code>classmethod</code>","text":"<p>Validate API key format based on provider.</p> Source code in <code>src/ice_sdk/models/config.py</code> <pre><code>@field_validator(\"api_key\")\n@classmethod\ndef validate_api_key(cls, v: Optional[str], info) -&gt; Optional[str]:\n    \"\"\"Validate API key format based on provider.\"\"\"\n    provider = info.data.get(\"provider\", ModelProvider.OPENAI)\n    if v is None:\n        # Allow missing api_key (will be loaded from env)\n        return v\n    if not v:  # All providers require an API key if provided\n        raise ValueError(f\"API key for provider {provider.value} cannot be empty.\")\n    # Allow test keys for any provider\n    if v.startswith(\"test-\"):\n        return v\n    if provider == ModelProvider.OPENAI:\n        if not (v.startswith(\"sk-\") and len(v) == 51) and not v.startswith(\n            \"sk-proj-\"\n        ):\n            logger.warning(\n                \"OpenAI API key does not match standard formats (sk-..., sk-proj-...). Proceeding, but please verify.\"\n            )\n    elif provider == ModelProvider.ANTHROPIC:\n        if not v.startswith(\"sk-ant-\"):\n            logger.warning(\n                \"Anthropic API key does not start with 'sk-ant-'. Proceeding, but please verify.\"\n            )\n    elif provider == ModelProvider.GOOGLE:\n        if len(v) &lt; 30:\n            logger.warning(\n                \"Google (Gemini) API key seems short. Proceeding, but please verify.\"\n            )\n    elif provider == ModelProvider.DEEPSEEK:\n        if not v.startswith(\"sk-\"):\n            logger.warning(\n                \"DeepSeek API key does not start with 'sk-'. Proceeding, but please verify its format.\"\n            )\n    elif provider == ModelProvider.CUSTOM:\n        pass\n    return v\n</code></pre>"},{"location":"api_reference/#ice_sdk.LLMConfig.validate_model","title":"<code>validate_model(v, info)</code>  <code>classmethod</code>","text":"<p>Validate model name based on provider.</p> Source code in <code>src/ice_sdk/models/config.py</code> <pre><code>@field_validator(\"model\")\n@classmethod\ndef validate_model(cls, v: str, info) -&gt; str:\n    \"\"\"Validate model name based on provider.\"\"\"\n    provider = info.data.get(\"provider\", ModelProvider.OPENAI)\n    try:\n        parse_model_version(v, provider)\n        return v\n    except ValueError as e:\n        raise ValueError(f\"Invalid model for provider {provider}: {str(e)}\")\n</code></pre>"},{"location":"api_reference/#ice_sdk.MessageTemplate","title":"<code>MessageTemplate</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Template for message generation</p> Source code in <code>src/ice_sdk/models/config.py</code> <pre><code>class MessageTemplate(BaseModel):\n    \"\"\"Template for message generation\"\"\"\n\n    role: str = Field(..., description=\"Message role (system, user, assistant)\")\n    content: str = Field(..., description=\"Message content template\")\n    version: str = Field(\n        \"1.0.0\", pattern=r\"^\\d+\\.\\d+\\.\\d+$\", description=\"Template version\"\n    )\n    min_model_version: str = Field(\n        \"gpt-4\", description=\"Minimum required model version\"\n    )\n    provider: ModelProvider = Field(\n        ModelProvider.OPENAI, description=\"Model provider for this template\"\n    )\n\n    def format(self, **kwargs) -&gt; str:\n        \"\"\"Format template with provided values, using defaults for missing keys\"\"\"\n        try:\n            return self.content.format(**kwargs)\n        except KeyError as e:\n            # Return unformatted content if formatting fails\n            print(f\"Warning: Missing template key {e}, using unformatted content\")\n            return self.content\n\n    model_config = ConfigDict(extra=\"forbid\")\n\n    @field_validator(\"role\")\n    @classmethod\n    def validate_role(cls, v: str) -&gt; str:\n        \"\"\"Validate message role\"\"\"\n        valid_roles = [\"system\", \"user\", \"assistant\"]\n        if v not in valid_roles:\n            raise ValueError(f\"Invalid role. Valid roles: {', '.join(valid_roles)}\")\n        return v\n\n    @field_validator(\"version\")\n    @classmethod\n    def validate_version_format(cls, v: str) -&gt; str:\n        \"\"\"Validate version format\"\"\"\n        if not re.match(r\"^\\d+\\.\\d+\\.\\d+$\", v):\n            raise ValueError(\"Version must use semantic format (e.g., 1.2.3)\")\n        return v\n\n    @field_validator(\"min_model_version\")\n    @classmethod\n    def validate_model_version(cls, v: str, info) -&gt; str:\n        \"\"\"Validate model version\"\"\"\n        provider = info.data.get(\"provider\", ModelProvider.OPENAI)\n        try:\n            parse_model_version(v, provider)\n            return v\n        except ValueError as e:\n            raise ValueError(f\"Invalid model version for provider {provider}: {str(e)}\")\n\n    def is_compatible_with_model(\n        self, model_name: str, provider: ModelProvider = ModelProvider.OPENAI\n    ) -&gt; bool:\n        \"\"\"Check if template is compatible with given model version.\n\n        Args:\n            model_name: Name of the model to check compatibility with\n            provider: Model provider to use for version checking\n\n        Returns:\n            True if model meets minimum version requirement, False otherwise\n        \"\"\"\n        try:\n            model_ver = version.parse(parse_model_version(model_name, provider))\n            min_ver = version.parse(\n                parse_model_version(self.min_model_version, self.provider)\n            )\n            return model_ver &gt;= min_ver\n        except ValueError:\n            return False\n\n    def __init__(self, **data):\n        super().__init__(**data)\n        # Validate model version compatibility during initialization\n        if not self.is_compatible_with_model(\n            data.get(\"min_model_version\", \"gpt-4\"),\n            data.get(\"provider\", ModelProvider.OPENAI),\n        ):\n            raise ValueError(\n                f\"Model {data.get('min_model_version')} is too old for this template\"\n            )\n</code></pre>"},{"location":"api_reference/#ice_sdk.MessageTemplate.format","title":"<code>format(**kwargs)</code>","text":"<p>Format template with provided values, using defaults for missing keys</p> Source code in <code>src/ice_sdk/models/config.py</code> <pre><code>def format(self, **kwargs) -&gt; str:\n    \"\"\"Format template with provided values, using defaults for missing keys\"\"\"\n    try:\n        return self.content.format(**kwargs)\n    except KeyError as e:\n        # Return unformatted content if formatting fails\n        print(f\"Warning: Missing template key {e}, using unformatted content\")\n        return self.content\n</code></pre>"},{"location":"api_reference/#ice_sdk.MessageTemplate.is_compatible_with_model","title":"<code>is_compatible_with_model(model_name, provider=ModelProvider.OPENAI)</code>","text":"<p>Check if template is compatible with given model version.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the model to check compatibility with</p> required <code>provider</code> <code>ModelProvider</code> <p>Model provider to use for version checking</p> <code>OPENAI</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if model meets minimum version requirement, False otherwise</p> Source code in <code>src/ice_sdk/models/config.py</code> <pre><code>def is_compatible_with_model(\n    self, model_name: str, provider: ModelProvider = ModelProvider.OPENAI\n) -&gt; bool:\n    \"\"\"Check if template is compatible with given model version.\n\n    Args:\n        model_name: Name of the model to check compatibility with\n        provider: Model provider to use for version checking\n\n    Returns:\n        True if model meets minimum version requirement, False otherwise\n    \"\"\"\n    try:\n        model_ver = version.parse(parse_model_version(model_name, provider))\n        min_ver = version.parse(\n            parse_model_version(self.min_model_version, self.provider)\n        )\n        return model_ver &gt;= min_ver\n    except ValueError:\n        return False\n</code></pre>"},{"location":"api_reference/#ice_sdk.MessageTemplate.validate_model_version","title":"<code>validate_model_version(v, info)</code>  <code>classmethod</code>","text":"<p>Validate model version</p> Source code in <code>src/ice_sdk/models/config.py</code> <pre><code>@field_validator(\"min_model_version\")\n@classmethod\ndef validate_model_version(cls, v: str, info) -&gt; str:\n    \"\"\"Validate model version\"\"\"\n    provider = info.data.get(\"provider\", ModelProvider.OPENAI)\n    try:\n        parse_model_version(v, provider)\n        return v\n    except ValueError as e:\n        raise ValueError(f\"Invalid model version for provider {provider}: {str(e)}\")\n</code></pre>"},{"location":"api_reference/#ice_sdk.MessageTemplate.validate_role","title":"<code>validate_role(v)</code>  <code>classmethod</code>","text":"<p>Validate message role</p> Source code in <code>src/ice_sdk/models/config.py</code> <pre><code>@field_validator(\"role\")\n@classmethod\ndef validate_role(cls, v: str) -&gt; str:\n    \"\"\"Validate message role\"\"\"\n    valid_roles = [\"system\", \"user\", \"assistant\"]\n    if v not in valid_roles:\n        raise ValueError(f\"Invalid role. Valid roles: {', '.join(valid_roles)}\")\n    return v\n</code></pre>"},{"location":"api_reference/#ice_sdk.MessageTemplate.validate_version_format","title":"<code>validate_version_format(v)</code>  <code>classmethod</code>","text":"<p>Validate version format</p> Source code in <code>src/ice_sdk/models/config.py</code> <pre><code>@field_validator(\"version\")\n@classmethod\ndef validate_version_format(cls, v: str) -&gt; str:\n    \"\"\"Validate version format\"\"\"\n    if not re.match(r\"^\\d+\\.\\d+\\.\\d+$\", v):\n        raise ValueError(\"Version must use semantic format (e.g., 1.2.3)\")\n    return v\n</code></pre>"},{"location":"api_reference/#ice_sdk.NodeExecutionResult","title":"<code>NodeExecutionResult</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Result of a node execution.</p> Source code in <code>src/ice_sdk/models/node_models.py</code> <pre><code>class NodeExecutionResult(BaseModel):\n    \"\"\"Result of a node execution.\"\"\"\n\n    success: bool = Field(\n        default=True, description=\"Whether the execution was successful\"\n    )\n    error: Optional[str] = Field(None, description=\"Error message if execution failed\")\n    output: Optional[Any] = Field(\n        None, description=\"Output data from the node (dict, str, etc.)\"\n    )\n    metadata: NodeMetadata = Field(..., description=\"Metadata about the execution\")\n    usage: Optional[UsageMetadata] = Field(\n        None, description=\"Usage statistics from the execution\"\n    )\n    execution_time: Optional[float] = Field(\n        None, description=\"Execution time in seconds\"\n    )\n    context_used: Optional[Dict[str, Any]] = Field(\n        None, description=\"Context used for the execution\"\n    )\n    token_stats: Optional[Dict[str, Any]] = Field(\n        None, description=\"Token statistics including truncation and limits\"\n    )\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n</code></pre>"},{"location":"api_reference/#ice_sdk.NodeMetadata","title":"<code>NodeMetadata</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Metadata model for node versioning and ownership</p> Source code in <code>src/ice_sdk/models/node_models.py</code> <pre><code>class NodeMetadata(BaseModel):\n    \"\"\"Metadata model for node versioning and ownership\"\"\"\n\n    node_id: str = Field(..., description=\"Unique node identifier\")\n    node_type: str = Field(..., description=\"Type of node (ai)\")\n    name: Optional[str] = None\n    version: str = Field(\n        \"1.0.0\",\n        pattern=r\"^\\d+\\.\\d+\\.\\d+$\",\n        description=\"Semantic version of node configuration\",\n    )\n    owner: Optional[str] = Field(None, description=\"Node owner/maintainer\")\n    created_at: Optional[datetime] = None\n    modified_at: Optional[datetime] = None\n    description: Optional[str] = Field(None, description=\"Description of the node\")\n    error_type: Optional[str] = Field(\n        None, description=\"Type of error if execution failed\"\n    )\n    timestamp: Optional[datetime] = None\n    start_time: Optional[datetime] = Field(None, description=\"Execution start time\")\n    end_time: Optional[datetime] = Field(None, description=\"Execution end time\")\n    duration: Optional[float] = Field(None, description=\"Execution duration in seconds\")\n    provider: Optional[ModelProvider] = Field(\n        None, description=\"LLM provider used for execution\"\n    )\n    retry_count: int = Field(\n        default=0,\n        ge=0,\n        description=\"Number of retry attempts performed during node execution\",\n    )\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def set_modified_at(cls, values):\n        \"\"\"Update modified_at timestamp on any change.\"\"\"\n        values[\"modified_at\"] = datetime.utcnow()\n        return values\n\n    # Automatically compute duration if possible ------------------------------\n    @model_validator(mode=\"after\")\n    def _set_duration(self):  # noqa: D401\n        if self.start_time and self.end_time and self.duration is None:\n            self.duration = (self.end_time - self.start_time).total_seconds()\n        elif self.start_time and self.duration is not None and self.end_time is None:\n            from datetime import timedelta\n\n            self.end_time = self.start_time + timedelta(seconds=self.duration)\n        return self\n</code></pre>"},{"location":"api_reference/#ice_sdk.NodeMetadata.set_modified_at","title":"<code>set_modified_at(values)</code>  <code>classmethod</code>","text":"<p>Update modified_at timestamp on any change.</p> Source code in <code>src/ice_sdk/models/node_models.py</code> <pre><code>@model_validator(mode=\"before\")\n@classmethod\ndef set_modified_at(cls, values):\n    \"\"\"Update modified_at timestamp on any change.\"\"\"\n    values[\"modified_at\"] = datetime.utcnow()\n    return values\n</code></pre>"},{"location":"api_reference/#ice_sdk.ToolService","title":"<code>ToolService</code>","text":"Source code in <code>src/ice_sdk/tools/service.py</code> <pre><code>class ToolService:  # noqa: D101 \u2013 simple fa\u00e7ade\n    _registry: Dict[str, Type[BaseTool]]\n\n    # ------------------------------------------------------------------\n    # Construction &amp; discovery -----------------------------------------\n    # ------------------------------------------------------------------\n    def __init__(self, auto_register_builtins: bool = True) -&gt; None:\n        \"\"\"Create a new *ToolService* instance.\n\n        Args:\n            auto_register_builtins: If *True* (default) the constructor will\n                automatically register all built-in tools shipped with\n                *ice_sdk* so they are readily available.\n        \"\"\"\n\n        self._registry = {}\n        if auto_register_builtins:\n            self._register_default_tools()\n\n    # ------------------------------------------------------------------\n    # New feature: file-system discovery of ``*.tool.py`` ----------------\n    # ------------------------------------------------------------------\n\n    def discover_and_register(self, directory: str | Path = \".\", pattern: str = \"*.tool.py\") -&gt; None:\n        \"\"\"Recursively import and register any *tool* modules found under *directory*.\n\n        This implements the Day-3 \"auto-registration of `*.tool.py` files\" milestone.\n\n        The function is *idempotent*: re-discovering the same directory twice\n        will not raise \u2013 duplicate tool names are ignored with a warning.\n        \"\"\"\n\n        import importlib\n        import inspect\n        import logging\n\n        logger = logging.getLogger(__name__)\n\n        base_path = Path(directory).resolve()\n\n        import sys\n        if str(base_path) not in sys.path:\n            sys.path.insert(0, str(base_path))\n\n        for py_file in base_path.rglob(pattern):\n            # Compute importable module path (assuming it is on sys.path)\n            try:\n                rel_path = py_file.relative_to(base_path)\n            except ValueError:\n                rel_path = py_file.name  # type: ignore[assignment]\n\n            module_name = Path(str(rel_path)).with_suffix(\"\").as_posix().replace(\"/\", \".\")\n\n            try:\n                # Attempt normal import first --------------------------------\n                try:\n                    module = importlib.import_module(module_name)\n                except ModuleNotFoundError:\n                    # Fall back to loading directly from file path ----------\n                    import importlib.util\n                    spec = importlib.util.spec_from_file_location(module_name.replace(\".\", \"_\"), py_file)\n                    if spec and spec.loader:\n                        module = importlib.util.module_from_spec(spec)\n                        spec.loader.exec_module(module)  # type: ignore[reportGeneralTypeIssues]\n                    else:\n                        raise\n            except Exception as exc:\n                logger.warning(\"Could not import %s: %s\", module_name, exc)\n                continue\n\n            # Inspect module attributes for *tool* subclasses -------------\n            for obj in module.__dict__.values():\n                if inspect.isclass(obj) and issubclass(obj, BaseTool):\n                    tool_name = getattr(obj, \"name\", None)\n                    try:\n                        self.register(obj)\n                        logger.info(\"Registered tool '%s' from %s\", tool_name, module_name)\n                    except ValueError:\n                        # Duplicate registration \u2013 ignore silently so repeated scans are safe.\n                        continue\n\n    # ---------------------------------------------------------------------\n    # Public API\n    # ---------------------------------------------------------------------\n    def register(self, tool_cls: Type[BaseTool]) -&gt; None:\n        \"\"\"Register a new *tool class* so it can be instantiated by name.\"\"\"\n        if not issubclass(tool_cls, BaseTool):\n            raise TypeError(\"tool_cls must inherit from BaseTool\")\n        if not getattr(tool_cls, \"name\", None):\n            raise ValueError(\"Tool classes must define a unique `name` attribute\")\n        self._registry[tool_cls.name] = tool_cls\n\n    def get(self, name: str, **init_kwargs: Any) -&gt; BaseTool:\n        \"\"\"Instantiate and return the tool identified by *name*.\"\"\"\n        try:\n            tool_cls = self._registry[name]\n        except KeyError as exc:  # pragma: no cover \u2013 defensive branch\n            raise KeyError(f\"Tool '{name}' is not registered\") from exc\n        return tool_cls(**init_kwargs)  # type: ignore[call-arg]\n\n    def available_tools(self) -&gt; Iterable[str]:  # noqa: D401\n        \"\"\"Return an *iterator* over the names of registered tools.\"\"\"\n        return self._registry.keys()\n\n    # ------------------------------------------------------------------\n    # Capability cards --------------------------------------------------\n    # ------------------------------------------------------------------\n\n    def cards(self) -&gt; Iterable[\"CapabilityCard\"]:  # noqa: D401\n        \"\"\"Yield :class:`~ice_sdk.capabilities.card.CapabilityCard` for every registered tool.\n\n        The method is intentionally *lazy* (uses a generator) so callers can\n        iterate without materialising the full list.\n        \"\"\"\n\n        # Local import to avoid an *optional* dependency when callers never\n        # request capability cards.\n        from ice_sdk.capabilities.card import CapabilityCard\n\n        for tool_cls in self._registry.values():\n            yield CapabilityCard.from_tool_cls(tool_cls)\n\n    # ------------------------------------------------------------------\n    # Internals\n    # ------------------------------------------------------------------\n    def _register_default_tools(self) -&gt; None:\n        \"\"\"Register built-in tools shipped with *ice_sdk*.\"\"\"\n        # Import *inside* the method to avoid potential circular imports if\n        # user code also imports from `ice_sdk` at module top-level.\n        from .builtins import HttpRequestTool, SleepTool, SumTool  # noqa: WPS433\n        from .hosted import ComputerTool, FileSearchTool, WebSearchTool  # noqa: WPS433\n\n        for tool_cls in (\n            WebSearchTool,\n            FileSearchTool,\n            ComputerTool,\n            SleepTool,\n            HttpRequestTool,\n            SumTool,\n        ):\n            try:\n                self.register(tool_cls)\n            except Exception:  # noqa: BLE001 \u2013 best-effort registration\n                # If a single tool fails to register (e.g. due to missing deps)\n                # we ignore it so *ToolService* still provides the others.\n                continue \n</code></pre>"},{"location":"api_reference/#ice_sdk.ToolService.__init__","title":"<code>__init__(auto_register_builtins=True)</code>","text":"<p>Create a new ToolService instance.</p> <p>Parameters:</p> Name Type Description Default <code>auto_register_builtins</code> <code>bool</code> <p>If True (default) the constructor will automatically register all built-in tools shipped with ice_sdk so they are readily available.</p> <code>True</code> Source code in <code>src/ice_sdk/tools/service.py</code> <pre><code>def __init__(self, auto_register_builtins: bool = True) -&gt; None:\n    \"\"\"Create a new *ToolService* instance.\n\n    Args:\n        auto_register_builtins: If *True* (default) the constructor will\n            automatically register all built-in tools shipped with\n            *ice_sdk* so they are readily available.\n    \"\"\"\n\n    self._registry = {}\n    if auto_register_builtins:\n        self._register_default_tools()\n</code></pre>"},{"location":"api_reference/#ice_sdk.ToolService.available_tools","title":"<code>available_tools()</code>","text":"<p>Return an iterator over the names of registered tools.</p> Source code in <code>src/ice_sdk/tools/service.py</code> <pre><code>def available_tools(self) -&gt; Iterable[str]:  # noqa: D401\n    \"\"\"Return an *iterator* over the names of registered tools.\"\"\"\n    return self._registry.keys()\n</code></pre>"},{"location":"api_reference/#ice_sdk.ToolService.cards","title":"<code>cards()</code>","text":"<p>Yield :class:<code>~ice_sdk.capabilities.card.CapabilityCard</code> for every registered tool.</p> <p>The method is intentionally lazy (uses a generator) so callers can iterate without materialising the full list.</p> Source code in <code>src/ice_sdk/tools/service.py</code> <pre><code>def cards(self) -&gt; Iterable[\"CapabilityCard\"]:  # noqa: D401\n    \"\"\"Yield :class:`~ice_sdk.capabilities.card.CapabilityCard` for every registered tool.\n\n    The method is intentionally *lazy* (uses a generator) so callers can\n    iterate without materialising the full list.\n    \"\"\"\n\n    # Local import to avoid an *optional* dependency when callers never\n    # request capability cards.\n    from ice_sdk.capabilities.card import CapabilityCard\n\n    for tool_cls in self._registry.values():\n        yield CapabilityCard.from_tool_cls(tool_cls)\n</code></pre>"},{"location":"api_reference/#ice_sdk.ToolService.discover_and_register","title":"<code>discover_and_register(directory='.', pattern='*.tool.py')</code>","text":"<p>Recursively import and register any tool modules found under directory.</p> <p>This implements the Day-3 \"auto-registration of <code>*.tool.py</code> files\" milestone.</p> <p>The function is idempotent: re-discovering the same directory twice will not raise \u2013 duplicate tool names are ignored with a warning.</p> Source code in <code>src/ice_sdk/tools/service.py</code> <pre><code>def discover_and_register(self, directory: str | Path = \".\", pattern: str = \"*.tool.py\") -&gt; None:\n    \"\"\"Recursively import and register any *tool* modules found under *directory*.\n\n    This implements the Day-3 \"auto-registration of `*.tool.py` files\" milestone.\n\n    The function is *idempotent*: re-discovering the same directory twice\n    will not raise \u2013 duplicate tool names are ignored with a warning.\n    \"\"\"\n\n    import importlib\n    import inspect\n    import logging\n\n    logger = logging.getLogger(__name__)\n\n    base_path = Path(directory).resolve()\n\n    import sys\n    if str(base_path) not in sys.path:\n        sys.path.insert(0, str(base_path))\n\n    for py_file in base_path.rglob(pattern):\n        # Compute importable module path (assuming it is on sys.path)\n        try:\n            rel_path = py_file.relative_to(base_path)\n        except ValueError:\n            rel_path = py_file.name  # type: ignore[assignment]\n\n        module_name = Path(str(rel_path)).with_suffix(\"\").as_posix().replace(\"/\", \".\")\n\n        try:\n            # Attempt normal import first --------------------------------\n            try:\n                module = importlib.import_module(module_name)\n            except ModuleNotFoundError:\n                # Fall back to loading directly from file path ----------\n                import importlib.util\n                spec = importlib.util.spec_from_file_location(module_name.replace(\".\", \"_\"), py_file)\n                if spec and spec.loader:\n                    module = importlib.util.module_from_spec(spec)\n                    spec.loader.exec_module(module)  # type: ignore[reportGeneralTypeIssues]\n                else:\n                    raise\n        except Exception as exc:\n            logger.warning(\"Could not import %s: %s\", module_name, exc)\n            continue\n\n        # Inspect module attributes for *tool* subclasses -------------\n        for obj in module.__dict__.values():\n            if inspect.isclass(obj) and issubclass(obj, BaseTool):\n                tool_name = getattr(obj, \"name\", None)\n                try:\n                    self.register(obj)\n                    logger.info(\"Registered tool '%s' from %s\", tool_name, module_name)\n                except ValueError:\n                    # Duplicate registration \u2013 ignore silently so repeated scans are safe.\n                    continue\n</code></pre>"},{"location":"api_reference/#ice_sdk.ToolService.get","title":"<code>get(name, **init_kwargs)</code>","text":"<p>Instantiate and return the tool identified by name.</p> Source code in <code>src/ice_sdk/tools/service.py</code> <pre><code>def get(self, name: str, **init_kwargs: Any) -&gt; BaseTool:\n    \"\"\"Instantiate and return the tool identified by *name*.\"\"\"\n    try:\n        tool_cls = self._registry[name]\n    except KeyError as exc:  # pragma: no cover \u2013 defensive branch\n        raise KeyError(f\"Tool '{name}' is not registered\") from exc\n    return tool_cls(**init_kwargs)  # type: ignore[call-arg]\n</code></pre>"},{"location":"api_reference/#ice_sdk.ToolService.register","title":"<code>register(tool_cls)</code>","text":"<p>Register a new tool class so it can be instantiated by name.</p> Source code in <code>src/ice_sdk/tools/service.py</code> <pre><code>def register(self, tool_cls: Type[BaseTool]) -&gt; None:\n    \"\"\"Register a new *tool class* so it can be instantiated by name.\"\"\"\n    if not issubclass(tool_cls, BaseTool):\n        raise TypeError(\"tool_cls must inherit from BaseTool\")\n    if not getattr(tool_cls, \"name\", None):\n        raise ValueError(\"Tool classes must define a unique `name` attribute\")\n    self._registry[tool_cls.name] = tool_cls\n</code></pre>"},{"location":"api_reference/#ice_orchestrator_1","title":"ice_orchestrator","text":"<p>Ice Orchestrator package.</p> <p>This package provides workflow orchestration capabilities for iceOS.</p>"},{"location":"api_reference/#ice_orchestrator.BaseScriptChain","title":"<code>BaseScriptChain</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Abstract base class for all ScriptChain types. Defines the common interface and shared logic for workflow orchestration.</p> Source code in <code>src/ice_orchestrator/base_script_chain.py</code> <pre><code>class BaseScriptChain(ABC):\n    \"\"\"\n    Abstract base class for all ScriptChain types.\n    Defines the common interface and shared logic for workflow orchestration.\n    \"\"\"\n\n    def __init__(\n        self,\n        nodes: List[NodeConfig],\n        name: Optional[str] = None,\n        context_manager: Optional[GraphContextManager] = None,\n        callbacks: Optional[List[Any]] = None,\n        max_parallel: int = 5,\n        persist_intermediate_outputs: bool = True,\n        tools: Optional[List[BaseTool]] = None,\n        initial_context: Optional[Dict[str, Any]] = None,\n        workflow_context: Optional[WorkflowExecutionContext] = None,\n        failure_policy: FailurePolicy = FailurePolicy.CONTINUE_POSSIBLE,\n        *,\n        session_id: Optional[str] = None,\n        use_cache: bool = True,\n    ):\n        \"\"\"Initialize script chain.\n\n        Args:\n            nodes: List of node configurations\n            name: Chain name\n            context_manager: Context manager\n            callbacks: List of callbacks\n            max_parallel: Maximum parallel executions\n            persist_intermediate_outputs: Whether to persist outputs\n            tools: List of tools available to nodes\n            initial_context: Initial execution context\n            workflow_context: Workflow execution context\n            failure_policy: Failure handling policy\n            session_id: Optional session ID\n            use_cache: Whether to use cache\n        \"\"\"\n        self.session_id = session_id or uuid4().hex\n        self.use_cache = use_cache\n\n        self.nodes = {node.id: node for node in nodes}\n        self.name = name or \"script_chain\"\n        self.context_manager = context_manager or GraphContextManager()\n        self.max_parallel = max_parallel\n        self.persist_intermediate_outputs = persist_intermediate_outputs\n        self.callbacks = callbacks or []\n        self.workflow_context = workflow_context or WorkflowExecutionContext()\n        self.failure_policy = failure_policy\n\n        # Register tools\n        if tools:\n            for tool in tools:\n                self.context_manager.register_tool(tool)\n\n        # Set initial context \u2013 always create a workflow-scoped context\n        context_metadata = initial_context or {}\n        self.context_manager.set_context(\n            GraphContext(\n                session_id=self.session_id,\n                metadata=context_metadata,\n                execution_id=f\"{self.session_id}_{datetime.utcnow().isoformat()}\",\n            )\n        )\n\n    @abstractmethod\n    async def execute(self) -&gt; NodeExecutionResult:\n        \"\"\"Execute the workflow and return a NodeExecutionResult.\"\"\"\n        pass\n\n    @abstractmethod\n    def get_node_dependencies(self, node_id: str) -&gt; List[str]:\n        \"\"\"Get dependencies for a node.\"\"\"\n        pass\n\n    @abstractmethod\n    def get_node_dependents(self, node_id: str) -&gt; List[str]:\n        \"\"\"Get dependents for a node.\"\"\"\n        pass\n\n    @abstractmethod\n    def get_node_level(self, node_id: str) -&gt; int:\n        \"\"\"Get execution level for a node.\"\"\"\n        pass\n\n    @abstractmethod\n    def get_level_nodes(self, level: int) -&gt; List[str]:\n        \"\"\"Get nodes at a specific level.\"\"\"\n        pass\n\n    @abstractmethod\n    def get_metrics(self) -&gt; Dict[str, Any]:\n        \"\"\"Get execution metrics.\"\"\"\n        pass\n\n    @abstractmethod\n    async def execute_node(self, node_id: str, input_data: Dict[str, Any]) -&gt; NodeExecutionResult:\n        \"\"\"Execute a single node and return its :class:`NodeExecutionResult`.  Must be\n        implemented by concrete subclasses (e.g. :class:`ScriptChain`).\"\"\"\n</code></pre>"},{"location":"api_reference/#ice_orchestrator.BaseScriptChain.__init__","title":"<code>__init__(nodes, name=None, context_manager=None, callbacks=None, max_parallel=5, persist_intermediate_outputs=True, tools=None, initial_context=None, workflow_context=None, failure_policy=FailurePolicy.CONTINUE_POSSIBLE, *, session_id=None, use_cache=True)</code>","text":"<p>Initialize script chain.</p> <p>Parameters:</p> Name Type Description Default <code>nodes</code> <code>List[NodeConfig]</code> <p>List of node configurations</p> required <code>name</code> <code>Optional[str]</code> <p>Chain name</p> <code>None</code> <code>context_manager</code> <code>Optional[GraphContextManager]</code> <p>Context manager</p> <code>None</code> <code>callbacks</code> <code>Optional[List[Any]]</code> <p>List of callbacks</p> <code>None</code> <code>max_parallel</code> <code>int</code> <p>Maximum parallel executions</p> <code>5</code> <code>persist_intermediate_outputs</code> <code>bool</code> <p>Whether to persist outputs</p> <code>True</code> <code>tools</code> <code>Optional[List[BaseTool]]</code> <p>List of tools available to nodes</p> <code>None</code> <code>initial_context</code> <code>Optional[Dict[str, Any]]</code> <p>Initial execution context</p> <code>None</code> <code>workflow_context</code> <code>Optional[WorkflowExecutionContext]</code> <p>Workflow execution context</p> <code>None</code> <code>failure_policy</code> <code>FailurePolicy</code> <p>Failure handling policy</p> <code>CONTINUE_POSSIBLE</code> <code>session_id</code> <code>Optional[str]</code> <p>Optional session ID</p> <code>None</code> <code>use_cache</code> <code>bool</code> <p>Whether to use cache</p> <code>True</code> Source code in <code>src/ice_orchestrator/base_script_chain.py</code> <pre><code>def __init__(\n    self,\n    nodes: List[NodeConfig],\n    name: Optional[str] = None,\n    context_manager: Optional[GraphContextManager] = None,\n    callbacks: Optional[List[Any]] = None,\n    max_parallel: int = 5,\n    persist_intermediate_outputs: bool = True,\n    tools: Optional[List[BaseTool]] = None,\n    initial_context: Optional[Dict[str, Any]] = None,\n    workflow_context: Optional[WorkflowExecutionContext] = None,\n    failure_policy: FailurePolicy = FailurePolicy.CONTINUE_POSSIBLE,\n    *,\n    session_id: Optional[str] = None,\n    use_cache: bool = True,\n):\n    \"\"\"Initialize script chain.\n\n    Args:\n        nodes: List of node configurations\n        name: Chain name\n        context_manager: Context manager\n        callbacks: List of callbacks\n        max_parallel: Maximum parallel executions\n        persist_intermediate_outputs: Whether to persist outputs\n        tools: List of tools available to nodes\n        initial_context: Initial execution context\n        workflow_context: Workflow execution context\n        failure_policy: Failure handling policy\n        session_id: Optional session ID\n        use_cache: Whether to use cache\n    \"\"\"\n    self.session_id = session_id or uuid4().hex\n    self.use_cache = use_cache\n\n    self.nodes = {node.id: node for node in nodes}\n    self.name = name or \"script_chain\"\n    self.context_manager = context_manager or GraphContextManager()\n    self.max_parallel = max_parallel\n    self.persist_intermediate_outputs = persist_intermediate_outputs\n    self.callbacks = callbacks or []\n    self.workflow_context = workflow_context or WorkflowExecutionContext()\n    self.failure_policy = failure_policy\n\n    # Register tools\n    if tools:\n        for tool in tools:\n            self.context_manager.register_tool(tool)\n\n    # Set initial context \u2013 always create a workflow-scoped context\n    context_metadata = initial_context or {}\n    self.context_manager.set_context(\n        GraphContext(\n            session_id=self.session_id,\n            metadata=context_metadata,\n            execution_id=f\"{self.session_id}_{datetime.utcnow().isoformat()}\",\n        )\n    )\n</code></pre>"},{"location":"api_reference/#ice_orchestrator.BaseScriptChain.execute","title":"<code>execute()</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Execute the workflow and return a NodeExecutionResult.</p> Source code in <code>src/ice_orchestrator/base_script_chain.py</code> <pre><code>@abstractmethod\nasync def execute(self) -&gt; NodeExecutionResult:\n    \"\"\"Execute the workflow and return a NodeExecutionResult.\"\"\"\n    pass\n</code></pre>"},{"location":"api_reference/#ice_orchestrator.BaseScriptChain.execute_node","title":"<code>execute_node(node_id, input_data)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Execute a single node and return its :class:<code>NodeExecutionResult</code>.  Must be implemented by concrete subclasses (e.g. :class:<code>ScriptChain</code>).</p> Source code in <code>src/ice_orchestrator/base_script_chain.py</code> <pre><code>@abstractmethod\nasync def execute_node(self, node_id: str, input_data: Dict[str, Any]) -&gt; NodeExecutionResult:\n    \"\"\"Execute a single node and return its :class:`NodeExecutionResult`.  Must be\n    implemented by concrete subclasses (e.g. :class:`ScriptChain`).\"\"\"\n</code></pre>"},{"location":"api_reference/#ice_orchestrator.BaseScriptChain.get_level_nodes","title":"<code>get_level_nodes(level)</code>  <code>abstractmethod</code>","text":"<p>Get nodes at a specific level.</p> Source code in <code>src/ice_orchestrator/base_script_chain.py</code> <pre><code>@abstractmethod\ndef get_level_nodes(self, level: int) -&gt; List[str]:\n    \"\"\"Get nodes at a specific level.\"\"\"\n    pass\n</code></pre>"},{"location":"api_reference/#ice_orchestrator.BaseScriptChain.get_metrics","title":"<code>get_metrics()</code>  <code>abstractmethod</code>","text":"<p>Get execution metrics.</p> Source code in <code>src/ice_orchestrator/base_script_chain.py</code> <pre><code>@abstractmethod\ndef get_metrics(self) -&gt; Dict[str, Any]:\n    \"\"\"Get execution metrics.\"\"\"\n    pass\n</code></pre>"},{"location":"api_reference/#ice_orchestrator.BaseScriptChain.get_node_dependencies","title":"<code>get_node_dependencies(node_id)</code>  <code>abstractmethod</code>","text":"<p>Get dependencies for a node.</p> Source code in <code>src/ice_orchestrator/base_script_chain.py</code> <pre><code>@abstractmethod\ndef get_node_dependencies(self, node_id: str) -&gt; List[str]:\n    \"\"\"Get dependencies for a node.\"\"\"\n    pass\n</code></pre>"},{"location":"api_reference/#ice_orchestrator.BaseScriptChain.get_node_dependents","title":"<code>get_node_dependents(node_id)</code>  <code>abstractmethod</code>","text":"<p>Get dependents for a node.</p> Source code in <code>src/ice_orchestrator/base_script_chain.py</code> <pre><code>@abstractmethod\ndef get_node_dependents(self, node_id: str) -&gt; List[str]:\n    \"\"\"Get dependents for a node.\"\"\"\n    pass\n</code></pre>"},{"location":"api_reference/#ice_orchestrator.BaseScriptChain.get_node_level","title":"<code>get_node_level(node_id)</code>  <code>abstractmethod</code>","text":"<p>Get execution level for a node.</p> Source code in <code>src/ice_orchestrator/base_script_chain.py</code> <pre><code>@abstractmethod\ndef get_node_level(self, node_id: str) -&gt; int:\n    \"\"\"Get execution level for a node.\"\"\"\n    pass\n</code></pre>"},{"location":"api_reference/#ice_orchestrator.ChainError","title":"<code>ChainError</code>","text":"<p>             Bases: <code>Exception</code></p> <p>Base exception class for ScriptChain errors</p> Source code in <code>src/ice_orchestrator/chain_errors.py</code> <pre><code>class ScriptChainError(Exception):\n    \"\"\"Base exception class for ScriptChain errors\"\"\"\n\n    pass\n</code></pre>"},{"location":"api_reference/#ice_orchestrator.DependencyGraph","title":"<code>DependencyGraph</code>","text":"<p>Handles dependency graph construction, cycle detection, level assignment, and queries for ScriptChain.</p> Source code in <code>src/ice_orchestrator/node_dependency_graph.py</code> <pre><code>class DependencyGraph:\n    \"\"\"\n    Handles dependency graph construction, cycle detection, level assignment, and queries for ScriptChain.\n    \"\"\"\n\n    def __init__(self, nodes: List[Any]):\n        self.graph = nx.DiGraph()\n        # Mapping of node_id -&gt; topological level (depth) ------------------\n        self.node_levels: Dict[str, int] = {}\n        self._build_graph(nodes)\n        self._assign_levels(nodes)\n\n    def _build_graph(self, nodes: List[Any]):\n        node_ids = {node.id for node in nodes}\n        for node in nodes:\n            self.graph.add_node(node.id, level=0)\n            for dep in getattr(node, \"dependencies\", []):\n                if dep not in node_ids:\n                    raise ValueError(f\"Dependency {dep} not found for node {node.id}\")\n                self.graph.add_edge(dep, node.id)\n        # Check for cycles\n        try:\n            cycles = list(nx.simple_cycles(self.graph))\n            if cycles:\n                cycle_str = \" -&gt; \".join(cycles[0])\n                raise CircularDependencyError(\n                    f\"Circular dependency detected: {cycle_str}\"\n                )\n        except nx.NetworkXNoCycle:\n            pass\n\n    def _assign_levels(self, nodes: List[Any]):\n        node_map = {node.id: node for node in nodes}\n        for node_id in nx.topological_sort(self.graph):\n            node = node_map[node_id]\n            node.level = (\n                max(\n                    (node_map[dep].level for dep in getattr(node, \"dependencies\", [])),\n                    default=-1,\n                )\n                + 1\n            )\n            self.node_levels[node_id] = node.level\n\n    def get_level_nodes(self) -&gt; Dict[int, List[str]]:\n        levels: Dict[int, List[str]] = {}\n        for node_id, level in self.node_levels.items():\n            if level not in levels:\n                levels[level] = []\n            levels[level].append(node_id)\n        return levels\n\n    def get_node_dependencies(self, node_id: str) -&gt; List[str]:\n        return list(self.graph.predecessors(node_id))\n\n    def get_node_dependents(self, node_id: str) -&gt; List[str]:\n        return list(self.graph.successors(node_id))\n\n    def get_node_level(self, node_id: str) -&gt; int:\n        return self.node_levels[node_id]\n\n    def get_leaf_nodes(self) -&gt; List[str]:\n        return [node for node, out_degree in self.graph.out_degree() if out_degree == 0]\n\n    def validate_schema_alignment(self, nodes: List[Any]):\n        node_map = {node.id: node for node in nodes}\n        for node in nodes:\n            # Check input mappings to ensure they reference valid dependencies and output keys\n            for placeholder, mapping in getattr(node, \"input_mappings\", {}).items():\n                dep_id = mapping.source_node_id\n                if dep_id not in self.get_node_dependencies(node.id):\n                    raise ValueError(\n                        f\"Node '{node.id}' has an input mapping for '{placeholder}' from '{dep_id}', which is not a direct dependency.\"\n                    )\n\n                dep_node = node_map.get(dep_id)\n                if not dep_node:\n                    raise ValueError(\n                        f\"Dependency node '{dep_id}' not found in the chain configuration.\"\n                    )\n\n                # Handle both Pydantic models and dicts for schema\n                if hasattr(dep_node.output_schema, \"model_fields\"):\n                    # It's a Pydantic model\n                    output_keys = dep_node.output_schema.model_fields.keys()\n                else:\n                    # It's a dictionary\n                    output_keys = getattr(dep_node, \"output_schema\", {}).keys()\n\n                # The source_output_key can be nested, e.g., 'data.result'.\n                # We'll check the top-level key for now.\n                top_level_key = mapping.source_output_key.split(\".\")[0]\n\n                if (\n                    top_level_key not in output_keys\n                    and mapping.source_output_key != \".\"\n                ):\n                    raise ValueError(\n                        f\"Node '{node.id}' expects input for '{placeholder}' from key '{mapping.source_output_key}' \"\n                        f\"of node '{dep_id}', but '{top_level_key}' is not in its output schema. \"\n                        f\"Available keys: {list(output_keys)}\"\n                    )\n</code></pre>"},{"location":"api_reference/#ice_orchestrator.FailurePolicy","title":"<code>FailurePolicy</code>","text":"<p>             Bases: <code>str</code>, <code>Enum</code></p> <p>Strategies controlling how the chain proceeds after node failures.</p> Source code in <code>src/ice_orchestrator/base_script_chain.py</code> <pre><code>class FailurePolicy(str, Enum):\n    \"\"\"Strategies controlling how the chain proceeds after node failures.\"\"\"\n\n    HALT = \"halt_on_first_error\"\n    CONTINUE_POSSIBLE = \"continue_if_possible\"\n    ALWAYS = \"always_continue\"\n</code></pre>"},{"location":"api_reference/#ice_orchestrator.ScriptChain","title":"<code>ScriptChain</code>","text":"<p>             Bases: <code>BaseScriptChain</code></p> <p>Execute a directed acyclic workflow using level-based parallelism.</p> <p>Nodes at the same topological level (i.e. depth in the dependency DAG) are executed concurrently up to the configured max_parallel limit.</p> <p>Features: - Level-based parallel execution - Robust error handling with configurable policies - Context &amp; state management - Performance features (caching, large output handling) - Observability (metrics, tracing, callbacks)</p> Source code in <code>src/ice_orchestrator/script_chain.py</code> <pre><code>class ScriptChain(BaseScriptChain):\n    \"\"\"Execute a directed acyclic workflow using level-based parallelism.\n\n    Nodes at the same topological level (i.e. depth in the dependency DAG)\n    are executed concurrently up to the configured max_parallel limit.\n\n    Features:\n    - Level-based parallel execution\n    - Robust error handling with configurable policies\n    - Context &amp; state management\n    - Performance features (caching, large output handling)\n    - Observability (metrics, tracing, callbacks)\n    \"\"\"\n\n    def __init__(\n        self,\n        nodes: List[NodeConfig],\n        name: Optional[str] = None,\n        *,\n        context_manager: Optional[GraphContextManager] = None,\n        callbacks: Optional[List[Any]] = None,\n        max_parallel: int = 5,\n        persist_intermediate_outputs: bool = True,\n        tools: Optional[List[BaseTool]] = None,\n        initial_context: Optional[Dict[str, Any]] = None,\n        workflow_context: Optional[WorkflowExecutionContext] = None,\n        chain_id: Optional[str] = None,\n        failure_policy: FailurePolicy = FailurePolicy.CONTINUE_POSSIBLE,\n        validate_outputs: bool = True,\n        token_ceiling: int | None = None,\n        depth_ceiling: int | None = None,\n        token_guard: TokenGuard | None = None,\n        depth_guard: DepthGuard | None = None,\n        session_id: Optional[str] = None,\n        use_cache: bool = True,\n    ) -&gt; None:\n        \"\"\"Initialize script chain.\n\n        Args:\n            nodes: List of node configurations\n            name: Chain name\n            context_manager: Context manager\n            callbacks: List of callbacks\n            max_parallel: Maximum parallel executions\n            persist_intermediate_outputs: Whether to persist outputs\n            tools: List of tools available to nodes\n            initial_context: Initial execution context\n            workflow_context: Workflow execution context\n            chain_id: Unique chain identifier\n            failure_policy: Failure handling policy\n            validate_outputs: Whether to validate node outputs\n            token_ceiling: Token ceiling for chain execution\n            depth_ceiling: Depth ceiling for chain execution\n            token_guard: Token guard for chain execution\n            depth_guard: Depth guard for chain execution\n            session_id: Session identifier\n            use_cache: Chain-level cache toggle\n        \"\"\"\n        self.chain_id = chain_id or f\"chain_{datetime.utcnow().isoformat()}\"\n        super().__init__(\n            nodes,\n            name,\n            context_manager,\n            callbacks,\n            max_parallel,\n            persist_intermediate_outputs,\n            tools,\n            initial_context,\n            workflow_context,\n            failure_policy,\n            session_id=session_id,\n            use_cache=use_cache,\n        )\n        self.validate_outputs = validate_outputs\n        self.use_cache = use_cache\n        self.token_ceiling = token_ceiling\n        self.depth_ceiling = depth_ceiling\n        # External guard callbacks --------------------------------------\n        self._token_guard = token_guard\n        self._depth_guard = depth_guard\n\n        # Build dependency graph\n        self.graph = DependencyGraph(nodes)\n        self.graph.validate_schema_alignment(nodes)\n        self.levels = self.graph.get_level_nodes()\n\n        # Metrics &amp; events\n        self.metrics = ChainMetrics()\n        # Agent instance cache -------------------------------------------\n        self._agent_cache: Dict[str, AgentNode] = {}\n        # Track decisions made by *condition* nodes -----------------------\n        self._branch_decisions: Dict[str, bool] = {}\n        # Retain reference to chain-level tools ---------------------------\n        self._chain_tools: List[BaseTool] = tools or []\n\n        from ice_sdk.cache import global_cache  # local import to avoid cycles\n        self._cache = global_cache()\n\n        logger.info(\n            \"Initialized ScriptChain with %d nodes across %d levels\",\n            len(nodes),\n            len(self.levels),\n        )\n\n    async def execute(self) -&gt; ChainExecutionResult:\n        \"\"\"Execute the workflow and return a ChainExecutionResult.\"\"\"\n        start_time = datetime.utcnow()\n        results: Dict[str, NodeExecutionResult] = {}\n        errors: List[str] = []\n\n        logger.info(\"Starting execution of chain '%s' (ID: %s)\", self.name, self.chain_id)\n\n        with tracer.start_as_current_span(\n            \"chain.execute\",\n            attributes={\n                \"chain_id\": self.chain_id,\n                \"chain_name\": self.name,\n                \"node_count\": len(self.nodes),\n            },\n        ) as chain_span:\n            for level_idx, level_num in enumerate(sorted(self.levels.keys()), start=1):\n\n                # External depth guard takes priority --------------------\n                if self._depth_guard and not self._depth_guard(level_idx, self.depth_ceiling):\n                    errors.append(\"Depth guard aborted execution\")\n                    break\n\n                if self.depth_ceiling is not None and level_idx &gt; self.depth_ceiling:\n                    logger.warning(\"Depth ceiling reached (%s); aborting further levels.\", self.depth_ceiling)\n                    errors.append(\"Depth ceiling reached\")\n                    break\n\n                level_node_ids = self.levels[level_num]\n                # Filter nodes by branch decisions (condition gating) -----\n                active_node_ids = [nid for nid in level_node_ids if self._is_node_active(nid)]\n                level_nodes = [self.nodes[node_id] for node_id in active_node_ids]\n\n                level_results = await self._execute_level(level_nodes, results)\n\n                for node_id, result in level_results.items():\n                    results[node_id] = result\n\n                    if result.success:\n                        if hasattr(result, \"usage\") and result.usage:\n                            self.metrics.update(node_id, result)\n\n                            # External token guard hook -------------------\n                            if self._token_guard and not self._token_guard(\n                                self.metrics.total_tokens, self.token_ceiling\n                            ):\n                                errors.append(\"Token guard aborted execution\")\n                                break\n\n                            # Token ceiling enforcement ----------------------\n                            if (\n                                self.token_ceiling is not None\n                                and self.metrics.total_tokens &gt; self.token_ceiling\n                            ):\n                                logger.warning(\n                                    \"Token ceiling exceeded (%s); aborting chain.\",\n                                    self.token_ceiling,\n                                )\n                                errors.append(\"Token ceiling exceeded\")\n                                break\n\n                    # ----------------------------------------------------------------------\n                    # Record branch decision for *condition* nodes (always, not usage-only)\n                    # ----------------------------------------------------------------------\n                    node_cfg = self.nodes[node_id]\n                    if (\n                        isinstance(node_cfg, ConditionNodeConfig)\n                        and isinstance(result.output, dict)\n                        and \"result\" in result.output\n                    ):\n                        try:\n                            self._branch_decisions[node_id] = bool(result.output[\"result\"])\n                        except Exception:\n                            # Defensive fallback \u2013 ignore unexpected conversion issues\n                            pass\n\n                    # When the node execution failed, collect error information\n                    if not result.success:\n                        errors.append(f\"Node {node_id} failed: {result.error}\")\n\n                if errors and not self._should_continue(errors):\n                    break\n\n            end_time = datetime.utcnow()\n            duration = (end_time - start_time).total_seconds()\n            logger.info(\n                \"Completed chain execution\", chain=self.name, chain_id=self.chain_id, duration=duration\n            )\n\n            chain_span.set_attribute(\"success\", len(errors) == 0)\n            if errors:\n                chain_span.set_status(Status(StatusCode.ERROR, \";\".join(errors)))\n            chain_span.end()\n\n        final_node_id = self.graph.get_leaf_nodes()[0]\n\n        return ChainExecutionResult(\n            success=len(errors) == 0,\n            output=results,\n            error=\"\\n\".join(errors) if errors else None,\n            metadata=NodeMetadata(\n                node_id=final_node_id,\n                node_type=\"script_chain\",\n                name=self.name,\n                version=\"1.0.0\",\n                start_time=start_time,\n                end_time=end_time,\n                duration=duration,\n            ),  # type: ignore[call-arg]\n            execution_time=duration,\n            token_stats=self.metrics.as_dict(),\n        )\n\n    async def _execute_level(\n        self,\n        level_nodes: List[NodeConfig],\n        accumulated_results: Dict[str, NodeExecutionResult],\n    ) -&gt; Dict[str, NodeExecutionResult]:\n        \"\"\"Execute all nodes at a given level in parallel.\"\"\"\n        semaphore = asyncio.Semaphore(self.max_parallel)\n\n        async def process_node(node: NodeConfig) -&gt; Tuple[str, NodeExecutionResult]:\n            weight = max(1, estimate_complexity(node))\n            async with WeightedSemaphore(semaphore, weight):\n                result = await self.execute_node(\n                    node.id,\n                    self._build_node_context(node, accumulated_results),\n                )\n                return node.id, result\n\n        tasks = [process_node(node) for node in level_nodes]\n        # Gather with *return_exceptions* so that a single node failure does not\n        # crash the entire level when *failure_policy* allows continuation.  Any\n        # exception is immediately converted into a failed *NodeExecutionResult*\n        # so downstream bookkeeping remains consistent.\n        gathered = await asyncio.gather(*tasks, return_exceptions=True)\n\n        level_results: Dict[str, NodeExecutionResult] = {}\n        for item in gathered:\n            if isinstance(item, tuple) and len(item) == 2:\n                node_id, result_or_exc = item\n\n                if isinstance(result_or_exc, Exception):\n                    # Convert the exception into a generic failure result so the\n                    # orchestrator can apply failure policies without blowing up.\n                    from datetime import datetime\n\n                    from ice_sdk.models.node_models import (\n                        NodeExecutionResult,\n                        NodeMetadata,\n                    )\n\n                    failure_meta = NodeMetadata(  # type: ignore[call-arg]\n                        node_id=node_id,\n                        node_type=\"unknown\",\n                        name=node_id,\n                        start_time=datetime.utcnow(),\n                        end_time=datetime.utcnow(),\n                        duration=0.0,\n                        error_type=type(result_or_exc).__name__,\n                    )\n\n                    level_results[node_id] = NodeExecutionResult(  # type: ignore[call-arg]\n                        success=False,\n                        error=str(result_or_exc),\n                        metadata=failure_meta,\n                    )\n                else:\n                    level_results[node_id] = result_or_exc\n            else:\n                # Defensive branch \u2014 should not happen but avoid silent loss.\n                import reprlib\n                from datetime import datetime\n\n                from ice_sdk.models.node_models import NodeExecutionResult, NodeMetadata\n\n                node_id = \"unknown\" if not item else str(item)\n                level_results[node_id] = NodeExecutionResult(  # type: ignore[call-arg]\n                    success=False,\n                    error=f\"Unexpected gather payload: {reprlib.repr(item)}\",\n                    metadata=NodeMetadata(  # type: ignore[call-arg]\n                        node_id=node_id,\n                        node_type=\"unknown\",\n                        name=node_id,\n                        start_time=datetime.utcnow(),\n                        end_time=datetime.utcnow(),\n                        duration=0.0,\n                        error_type=\"GatherPayloadError\",\n                    ),\n                )\n        return level_results\n\n    def _build_node_context(\n        self,\n        node: NodeConfig,\n        accumulated_results: Dict[str, NodeExecutionResult],\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Build execution context for a node.\"\"\"\n        context: Dict[str, Any] = {}\n        validation_errors: List[str] = []\n\n        if getattr(node, \"input_mappings\", None):\n            for placeholder, mapping in node.input_mappings.items():\n                # Support either raw dicts *or* InputMapping instances ----------------\n                if (\n                    (isinstance(mapping, dict) and \"source_node_id\" in mapping)\n                    or hasattr(mapping, \"source_node_id\")\n                ):\n                    dep_id = mapping[\"source_node_id\"] if isinstance(mapping, dict) else mapping.source_node_id  # type: ignore[index]\n                    output_key = mapping[\"source_output_key\"] if isinstance(mapping, dict) else mapping.source_output_key  # type: ignore[index]\n                    dep_result = accumulated_results.get(dep_id)\n\n                    if not dep_result or not dep_result.success:\n                        validation_errors.append(f\"Dependency '{dep_id}' failed or did not run.\")\n                        continue\n\n                    try:\n                        value = self._resolve_nested_path(dep_result.output, output_key)\n                        context[placeholder] = value\n                    except (KeyError, IndexError, TypeError) as exc:\n                        validation_errors.append(\n                            f\"Failed to resolve path '{output_key}' in dependency '{dep_id}': {exc}\"\n                        )\n                else:\n                    context[placeholder] = mapping  # fall back to raw value\n\n        if validation_errors:\n            raise ChainError(\n                f\"Node '{node.id}' context validation failed:\\n\" + \"\\n\".join(validation_errors)\n            )\n\n        return context\n\n    def _should_continue(self, errors: List[str]) -&gt; bool:\n        \"\"\"Determine whether chain execution should proceed after errors.\"\"\"\n        if not errors:\n            return True\n\n        if self.failure_policy == FailurePolicy.HALT:\n            return False\n        if self.failure_policy == FailurePolicy.ALWAYS:\n            return True\n\n        # CONTINUE_POSSIBLE\n        failed_nodes: Set[str] = set()\n        for error in errors:\n            if \"Node \" in error and \" failed:\" in error:\n                try:\n                    node_id = error.split(\"Node \")[1].split(\" failed:\")[0]\n                    failed_nodes.add(node_id)\n                except (IndexError, AttributeError):\n                    continue\n\n        for level_num in sorted(self.levels.keys()):\n            for node_id in self.levels[level_num]:\n                node = self.nodes[node_id]\n                if node_id in failed_nodes:\n                    continue\n                depends_on_failed_node = any(dep in failed_nodes for dep in getattr(node, \"dependencies\", []))\n                if not depends_on_failed_node:\n                    logger.info(\n                        \"Chain execution continuing: Node '%s' can still execute independently\",\n                        node_id,\n                    )\n                    return True\n\n        logger.warning(\n            \"Chain execution stopping: All remaining nodes depend on failed nodes: %s\",\n            failed_nodes,\n        )\n        return False\n\n    @staticmethod\n    def _resolve_nested_path(data: Any, path: str) -&gt; Any:\n        \"\"\"Resolve a dot-separated *path* in *data*.\n\n        Special cases\n        -------------\n        * ``path == \"\"`` or ``path == \".\"``  \u2192 return *data* unchanged.\n        \"\"\"\n        if not path or path == \".\":\n            return data\n        for key in path.split(\".\"):\n            if isinstance(data, dict):\n                data = data[key]\n            elif isinstance(data, list):\n                data = data[int(key)]\n            else:\n                raise TypeError(f\"Cannot resolve path '{path}' in {type(data)}\")\n        return data\n\n    # ---------------------------------------------------------------------\n    # Graph inspection public API -----------------------------------------\n    # ---------------------------------------------------------------------\n\n    def get_node_dependencies(self, node_id: str) -&gt; List[str]:\n        \"\"\"Get dependencies for a node.\"\"\"\n        return self.graph.get_node_dependencies(node_id)\n\n    def get_node_dependents(self, node_id: str) -&gt; List[str]:\n        \"\"\"Get dependents for a node.\"\"\"\n        return self.graph.get_node_dependents(node_id)\n\n    def get_node_level(self, node_id: str) -&gt; int:\n        \"\"\"Get execution level for a node.\"\"\"\n        return self.graph.get_node_level(node_id)\n\n    def get_level_nodes(self, level: int) -&gt; List[str]:\n        \"\"\"Get nodes at a specific level.\"\"\"\n        return self.levels.get(level, [])\n\n    def get_metrics(self) -&gt; Dict[str, Any]:\n        \"\"\"Get execution metrics.\"\"\"\n        return self.metrics.as_dict()\n\n    async def execute_node(self, node_id: str, input_data: Dict[str, Any]) -&gt; NodeExecutionResult:\n        \"\"\"Execute a single node using agent/tool wrappers (overrides BaseScriptChain).\"\"\"\n        node = self.nodes.get(node_id)\n        if node is None:\n            raise ValueError(f\"Node '{node_id}' not found in chain configuration\")\n\n        # ------------------------------------------------------------------\n        # Persist *input_data* to the context store ------------------------\n        # ------------------------------------------------------------------\n        self.context_manager.update_node_context(\n            node_id=node_id,\n            content=input_data,\n            execution_id=self.context_manager.get_context().execution_id  # type: ignore[attr-defined]\n        )\n\n        max_retries: int = int(getattr(node, \"retries\", 0))\n        base_backoff: float = float(getattr(node, \"backoff_seconds\", 0.0))\n\n        attempt = 0\n        last_error: Exception | None = None\n\n        while attempt &lt;= max_retries:\n            try:\n                # ------------------------------------------------------\n                # Cache lookup (opt-in) --------------------------------\n                # ------------------------------------------------------\n                import hashlib\n                import json\n\n                cache_key: str | None = None\n                if self.use_cache and getattr(node, \"use_cache\", True):\n                    try:\n                        # Include *node configuration* snapshot so that changes\n                        # (e.g. modified prompt) bust the cache automatically.\n                        from pydantic import BaseModel\n\n                        cfg_payload = (\n                            node.model_dump() if isinstance(node, BaseModel) else str(node)\n                        )\n\n                        payload = {\n                            \"node_id\": node_id,\n                            \"input\": input_data,\n                            \"cfg\": cfg_payload,\n                        }\n                        serialized = json.dumps(payload, sort_keys=True, default=str)\n                        cache_key = hashlib.sha256(serialized.encode()).hexdigest()\n                        cached = self._cache.get(cache_key)\n                        if cached is not None:\n                            return cached\n                    except Exception:\n                        # Fall back \u2013 never fail due to cache issues\n                        cache_key = None\n                # ----------------------------------------------------------\n                # Dispatch via the *Node Registry* -------------------------\n                # ----------------------------------------------------------\n                executor = get_executor(str(getattr(node, \"type\", \"\")))  # type: ignore[arg-type]\n                # ------------------------------------------------------\n                # Per-node tracing span --------------------------------\n                # ------------------------------------------------------\n                with tracer.start_as_current_span(\n                    \"node.execute\",\n                    attributes={\n                        \"node_id\": node_id,\n                        \"node_type\": str(getattr(node, \"type\", \"\")),\n                    },\n                ) as node_span:\n                    result = await executor(self, node, input_data)\n\n                    # Attach outcome metadata to the span ---------------\n                    node_span.set_attribute(\"success\", result.success)\n                    node_span.set_attribute(\"retry_count\", attempt)\n                    if not result.success:\n                        node_span.set_status(Status(StatusCode.ERROR, result.error or \"\"))\n\n                # Store in cache if enabled &amp; succeeded ------------------\n                if cache_key and result.success:\n                    self._cache.set(cache_key, result)\n\n                # Attach retry metadata -----------------------------------\n                if result.metadata:\n                    result.metadata.retry_count = attempt\n\n                # Persist *output* to the context store if configured ------\n                if self.persist_intermediate_outputs and result.output is not None:\n                    self.context_manager.update_node_context(\n                        node_id=node_id,\n                        content=result.output,\n                        execution_id=self.context_manager.get_context().execution_id  # type: ignore[attr-defined]\n                    )\n\n                # ----------------------------------------------------------\n                # Optional output validation -------------------------------\n                # ----------------------------------------------------------\n                if self.validate_outputs and getattr(node, \"output_schema\", None):\n                    if not self._is_output_valid(node, result.output):\n                        result.success = False\n                        err_msg = (\n                            f\"Output validation failed for node '{node_id}' against declared schema\"\n                        )\n                        result.error = (\n                            err_msg if result.error is None else result.error + \"; \" + err_msg\n                        )\n\n                return result\n\n            except Exception as e:  # pylint: disable=broad-except\n                last_error = e\n                if attempt &gt;= max_retries:\n                    break\n\n                # ------------------------------------------------------\n                # Exponential backoff before next retry ---------------\n                # ------------------------------------------------------\n                wait_seconds = base_backoff * (2 ** attempt) if base_backoff &gt; 0 else 0\n                if wait_seconds &gt; 0:\n                    await asyncio.sleep(wait_seconds)\n\n                attempt += 1\n\n        # ------------------------------------------------------------------\n        # All retries exhausted \u2013 return failure result ---------------------\n        # ------------------------------------------------------------------\n        from datetime import datetime\n\n        error_meta = NodeMetadata(\n            node_id=node_id,\n            node_type=str(getattr(node, \"type\", \"\")),\n            name=getattr(node, \"name\", None),\n            version=\"1.0.0\",\n            start_time=datetime.utcnow(),\n            end_time=datetime.utcnow(),\n            duration=0.0,\n            error_type=type(last_error).__name__ if last_error else \"UnknownError\",\n            retry_count=attempt,\n        )  # type: ignore[call-arg]\n\n        if self.failure_policy == FailurePolicy.HALT:\n            raise last_error if last_error else Exception(\"Unknown error\")\n\n        return NodeExecutionResult(  # type: ignore[call-arg]\n            success=False,\n            error=f\"Retry limit exceeded ({max_retries}) \u2013 last error: {last_error}\",\n            metadata=error_meta,\n        )\n\n    # ---------------------------------------------------------------------\n    # Internal helpers -----------------------------------------------------\n    # ---------------------------------------------------------------------\n\n    def _make_agent(self, node: AiNodeConfig) -&gt; AgentNode:\n        \"\"\"Convert an *AiNodeConfig* into a fully-initialised :class:`AgentNode`.\"\"\"\n        # Build tool map so later inserts override earlier ones (priority)\n        tool_map: Dict[str, BaseTool] = {}\n\n        # 1. Globally registered tools (lowest precedence) --------------\n        for name, tool in self.context_manager.get_all_tools().items():\n            tool_map[name] = tool\n\n        # 2. Chain-level tools \u2013 override globals when name clashes ------\n        for t in self._chain_tools:\n            tool_map[t.name] = t\n\n        # 3. Node-specific tool refs override everything else -----------\n        if getattr(node, \"tools\", None):\n            for cfg in node.tools:  # type: ignore[attr-defined]\n                t_obj = self.context_manager.get_tool(cfg.name)\n                if t_obj is not None:\n                    tool_map[t_obj.name] = t_obj\n\n        tools: List[BaseTool] = list(tool_map.values())\n\n        # Build AgentConfig ----------------------------------------------\n        model_settings = ModelSettings(\n            model=node.model,\n            temperature=getattr(node, \"temperature\", 0.7),\n            max_tokens=getattr(node, \"max_tokens\", None),\n            provider=str(getattr(node.provider, \"value\", node.provider)),\n        )\n\n        agent_cfg = AgentConfig(\n            name=node.name or node.id,\n            instructions=node.prompt,\n            model=node.model,\n            model_settings=model_settings,\n            tools=tools,\n        )  # type: ignore[call-arg]\n\n        agent = AgentNode(config=agent_cfg, context_manager=self.context_manager)\n        agent.tools = tools  # expose on instance (used by AgentNode.execute)\n\n        # ------------------------------------------------------------------\n        # Register agent &amp; tools with the ContextManager -------------------\n        # ------------------------------------------------------------------\n        try:\n            self.context_manager.register_agent(agent)\n        except ValueError:\n            # Already registered \u2013 ignore duplicate\n            pass\n\n        for tool in tools:\n            try:\n                self.context_manager.register_tool(tool)\n            except ValueError:\n                # Possible duplicate registration \u2013 safe to ignore\n                continue\n\n        return agent\n\n    @staticmethod\n    def _is_output_valid(node: NodeConfig, output: Any) -&gt; bool:  # noqa: D401\n        \"\"\"Validate *output* against ``node.output_schema``.  Returns *True* when\n        validation succeeds or no schema declared.\n\n        Supports both *dict*-based schemas and Pydantic ``BaseModel`` subclasses to\n        stay in sync with the flexible input validation strategy.\n        \"\"\"\n        schema = getattr(node, \"output_schema\", None)\n        if not schema:\n            return True\n\n        # ------------------------------------------------------------------\n        # 1. Pydantic model --------------------------------------------------\n        # ------------------------------------------------------------------\n        try:\n            from pydantic import BaseModel, ValidationError\n\n            if isinstance(schema, type) and issubclass(schema, BaseModel):\n                try:\n                    schema.model_validate(output)  # type: ignore[arg-type]\n                    return True\n                except ValidationError:\n                    return False\n        except Exception:\n            # Pydantic may not be importable in constrained envs \u2013 fall back.\n            pass\n\n        # ------------------------------------------------------------------\n        # 2. dict schema {key: type_str} ------------------------------------\n        # ------------------------------------------------------------------\n        if isinstance(schema, dict):\n            for key, type_str in schema.items():\n                if key not in output:\n                    return False\n                try:\n                    expected_type = eval(type_str)\n                except Exception:  # noqa: S110\n                    # Unsafe eval on trusted config; if fails assume pass-through.\n                    continue\n                if not isinstance(output[key], expected_type):\n                    return False\n            return True\n\n        # Unknown schema format \u2013 consider valid to avoid false negatives\n        return True\n\n    # ---------------------------------------------------------------------\n    # Branch gating helpers -------------------------------------------------\n    # ---------------------------------------------------------------------\n\n    def _is_node_active(self, node_id: str) -&gt; bool:  # noqa: D401\n        \"\"\"Determine whether *node_id* should run in the current execution.\n\n        The logic combines two independent gating mechanisms:\n\n        1. **Branch-based gating** \u2013 Nodes explicitly listed in a *Condition* node's\n           ``true_branch`` / ``false_branch`` lists are enabled or disabled based\n           on that condition's runtime decision.\n        2. **Dependency propagation** \u2013 If *any* direct or transitive dependency\n           has been disabled by step (1) (or by further propagation), the current\n           node is implicitly disabled as well.  This prevents nodes from running\n           with missing upstream context and avoids spurious validation errors\n           later in the pipeline.\n        \"\"\"\n\n        # ------------------------------------------------------------------\n        # 1. Explicit branch gating ----------------------------------------\n        # ------------------------------------------------------------------\n        from ice_sdk.models.node_models import ConditionNodeConfig  # local import\n\n        for cond_id, decision in self._branch_decisions.items():\n            cond_id_str = str(cond_id)\n            cond_cfg = self.nodes.get(cond_id_str)\n            if not isinstance(cond_cfg, ConditionNodeConfig):\n                continue\n\n            # Outcome TRUE \u2192 *false_branch* nodes are disabled -------------\n            if decision and cond_cfg.false_branch and node_id in cond_cfg.false_branch:\n                return False\n\n            # Outcome FALSE \u2192 *true_branch* nodes are disabled -------------\n            if not decision and cond_cfg.true_branch and node_id in cond_cfg.true_branch:\n                return False\n\n        # ------------------------------------------------------------------\n        # 2. Implicit propagation through dependencies ---------------------\n        # ------------------------------------------------------------------\n        # Cache already-computed decisions to avoid exponential recursion\n        if not hasattr(self, \"_active_cache\"):\n            self._active_cache: Dict[str, bool] = {}\n\n        if node_id in self._active_cache:\n            return self._active_cache[node_id]\n\n        deps = self.graph.get_node_dependencies(node_id)\n        for dep_id in deps:\n            if not self._is_node_active(dep_id):\n                self._active_cache[node_id] = False\n                return False\n\n        self._active_cache[node_id] = True\n        return True\n</code></pre>"},{"location":"api_reference/#ice_orchestrator.ScriptChain.__init__","title":"<code>__init__(nodes, name=None, *, context_manager=None, callbacks=None, max_parallel=5, persist_intermediate_outputs=True, tools=None, initial_context=None, workflow_context=None, chain_id=None, failure_policy=FailurePolicy.CONTINUE_POSSIBLE, validate_outputs=True, token_ceiling=None, depth_ceiling=None, token_guard=None, depth_guard=None, session_id=None, use_cache=True)</code>","text":"<p>Initialize script chain.</p> <p>Parameters:</p> Name Type Description Default <code>nodes</code> <code>List[NodeConfig]</code> <p>List of node configurations</p> required <code>name</code> <code>Optional[str]</code> <p>Chain name</p> <code>None</code> <code>context_manager</code> <code>Optional[GraphContextManager]</code> <p>Context manager</p> <code>None</code> <code>callbacks</code> <code>Optional[List[Any]]</code> <p>List of callbacks</p> <code>None</code> <code>max_parallel</code> <code>int</code> <p>Maximum parallel executions</p> <code>5</code> <code>persist_intermediate_outputs</code> <code>bool</code> <p>Whether to persist outputs</p> <code>True</code> <code>tools</code> <code>Optional[List[BaseTool]]</code> <p>List of tools available to nodes</p> <code>None</code> <code>initial_context</code> <code>Optional[Dict[str, Any]]</code> <p>Initial execution context</p> <code>None</code> <code>workflow_context</code> <code>Optional[WorkflowExecutionContext]</code> <p>Workflow execution context</p> <code>None</code> <code>chain_id</code> <code>Optional[str]</code> <p>Unique chain identifier</p> <code>None</code> <code>failure_policy</code> <code>FailurePolicy</code> <p>Failure handling policy</p> <code>CONTINUE_POSSIBLE</code> <code>validate_outputs</code> <code>bool</code> <p>Whether to validate node outputs</p> <code>True</code> <code>token_ceiling</code> <code>int | None</code> <p>Token ceiling for chain execution</p> <code>None</code> <code>depth_ceiling</code> <code>int | None</code> <p>Depth ceiling for chain execution</p> <code>None</code> <code>token_guard</code> <code>Any | None</code> <p>Token guard for chain execution</p> <code>None</code> <code>depth_guard</code> <code>Any | None</code> <p>Depth guard for chain execution</p> <code>None</code> <code>session_id</code> <code>Optional[str]</code> <p>Session identifier</p> <code>None</code> <code>use_cache</code> <code>bool</code> <p>Chain-level cache toggle</p> <code>True</code> Source code in <code>src/ice_orchestrator/script_chain.py</code> <pre><code>def __init__(\n    self,\n    nodes: List[NodeConfig],\n    name: Optional[str] = None,\n    *,\n    context_manager: Optional[GraphContextManager] = None,\n    callbacks: Optional[List[Any]] = None,\n    max_parallel: int = 5,\n    persist_intermediate_outputs: bool = True,\n    tools: Optional[List[BaseTool]] = None,\n    initial_context: Optional[Dict[str, Any]] = None,\n    workflow_context: Optional[WorkflowExecutionContext] = None,\n    chain_id: Optional[str] = None,\n    failure_policy: FailurePolicy = FailurePolicy.CONTINUE_POSSIBLE,\n    validate_outputs: bool = True,\n    token_ceiling: int | None = None,\n    depth_ceiling: int | None = None,\n    token_guard: TokenGuard | None = None,\n    depth_guard: DepthGuard | None = None,\n    session_id: Optional[str] = None,\n    use_cache: bool = True,\n) -&gt; None:\n    \"\"\"Initialize script chain.\n\n    Args:\n        nodes: List of node configurations\n        name: Chain name\n        context_manager: Context manager\n        callbacks: List of callbacks\n        max_parallel: Maximum parallel executions\n        persist_intermediate_outputs: Whether to persist outputs\n        tools: List of tools available to nodes\n        initial_context: Initial execution context\n        workflow_context: Workflow execution context\n        chain_id: Unique chain identifier\n        failure_policy: Failure handling policy\n        validate_outputs: Whether to validate node outputs\n        token_ceiling: Token ceiling for chain execution\n        depth_ceiling: Depth ceiling for chain execution\n        token_guard: Token guard for chain execution\n        depth_guard: Depth guard for chain execution\n        session_id: Session identifier\n        use_cache: Chain-level cache toggle\n    \"\"\"\n    self.chain_id = chain_id or f\"chain_{datetime.utcnow().isoformat()}\"\n    super().__init__(\n        nodes,\n        name,\n        context_manager,\n        callbacks,\n        max_parallel,\n        persist_intermediate_outputs,\n        tools,\n        initial_context,\n        workflow_context,\n        failure_policy,\n        session_id=session_id,\n        use_cache=use_cache,\n    )\n    self.validate_outputs = validate_outputs\n    self.use_cache = use_cache\n    self.token_ceiling = token_ceiling\n    self.depth_ceiling = depth_ceiling\n    # External guard callbacks --------------------------------------\n    self._token_guard = token_guard\n    self._depth_guard = depth_guard\n\n    # Build dependency graph\n    self.graph = DependencyGraph(nodes)\n    self.graph.validate_schema_alignment(nodes)\n    self.levels = self.graph.get_level_nodes()\n\n    # Metrics &amp; events\n    self.metrics = ChainMetrics()\n    # Agent instance cache -------------------------------------------\n    self._agent_cache: Dict[str, AgentNode] = {}\n    # Track decisions made by *condition* nodes -----------------------\n    self._branch_decisions: Dict[str, bool] = {}\n    # Retain reference to chain-level tools ---------------------------\n    self._chain_tools: List[BaseTool] = tools or []\n\n    from ice_sdk.cache import global_cache  # local import to avoid cycles\n    self._cache = global_cache()\n\n    logger.info(\n        \"Initialized ScriptChain with %d nodes across %d levels\",\n        len(nodes),\n        len(self.levels),\n    )\n</code></pre>"},{"location":"api_reference/#ice_orchestrator.ScriptChain.execute","title":"<code>execute()</code>  <code>async</code>","text":"<p>Execute the workflow and return a ChainExecutionResult.</p> Source code in <code>src/ice_orchestrator/script_chain.py</code> <pre><code>async def execute(self) -&gt; ChainExecutionResult:\n    \"\"\"Execute the workflow and return a ChainExecutionResult.\"\"\"\n    start_time = datetime.utcnow()\n    results: Dict[str, NodeExecutionResult] = {}\n    errors: List[str] = []\n\n    logger.info(\"Starting execution of chain '%s' (ID: %s)\", self.name, self.chain_id)\n\n    with tracer.start_as_current_span(\n        \"chain.execute\",\n        attributes={\n            \"chain_id\": self.chain_id,\n            \"chain_name\": self.name,\n            \"node_count\": len(self.nodes),\n        },\n    ) as chain_span:\n        for level_idx, level_num in enumerate(sorted(self.levels.keys()), start=1):\n\n            # External depth guard takes priority --------------------\n            if self._depth_guard and not self._depth_guard(level_idx, self.depth_ceiling):\n                errors.append(\"Depth guard aborted execution\")\n                break\n\n            if self.depth_ceiling is not None and level_idx &gt; self.depth_ceiling:\n                logger.warning(\"Depth ceiling reached (%s); aborting further levels.\", self.depth_ceiling)\n                errors.append(\"Depth ceiling reached\")\n                break\n\n            level_node_ids = self.levels[level_num]\n            # Filter nodes by branch decisions (condition gating) -----\n            active_node_ids = [nid for nid in level_node_ids if self._is_node_active(nid)]\n            level_nodes = [self.nodes[node_id] for node_id in active_node_ids]\n\n            level_results = await self._execute_level(level_nodes, results)\n\n            for node_id, result in level_results.items():\n                results[node_id] = result\n\n                if result.success:\n                    if hasattr(result, \"usage\") and result.usage:\n                        self.metrics.update(node_id, result)\n\n                        # External token guard hook -------------------\n                        if self._token_guard and not self._token_guard(\n                            self.metrics.total_tokens, self.token_ceiling\n                        ):\n                            errors.append(\"Token guard aborted execution\")\n                            break\n\n                        # Token ceiling enforcement ----------------------\n                        if (\n                            self.token_ceiling is not None\n                            and self.metrics.total_tokens &gt; self.token_ceiling\n                        ):\n                            logger.warning(\n                                \"Token ceiling exceeded (%s); aborting chain.\",\n                                self.token_ceiling,\n                            )\n                            errors.append(\"Token ceiling exceeded\")\n                            break\n\n                # ----------------------------------------------------------------------\n                # Record branch decision for *condition* nodes (always, not usage-only)\n                # ----------------------------------------------------------------------\n                node_cfg = self.nodes[node_id]\n                if (\n                    isinstance(node_cfg, ConditionNodeConfig)\n                    and isinstance(result.output, dict)\n                    and \"result\" in result.output\n                ):\n                    try:\n                        self._branch_decisions[node_id] = bool(result.output[\"result\"])\n                    except Exception:\n                        # Defensive fallback \u2013 ignore unexpected conversion issues\n                        pass\n\n                # When the node execution failed, collect error information\n                if not result.success:\n                    errors.append(f\"Node {node_id} failed: {result.error}\")\n\n            if errors and not self._should_continue(errors):\n                break\n\n        end_time = datetime.utcnow()\n        duration = (end_time - start_time).total_seconds()\n        logger.info(\n            \"Completed chain execution\", chain=self.name, chain_id=self.chain_id, duration=duration\n        )\n\n        chain_span.set_attribute(\"success\", len(errors) == 0)\n        if errors:\n            chain_span.set_status(Status(StatusCode.ERROR, \";\".join(errors)))\n        chain_span.end()\n\n    final_node_id = self.graph.get_leaf_nodes()[0]\n\n    return ChainExecutionResult(\n        success=len(errors) == 0,\n        output=results,\n        error=\"\\n\".join(errors) if errors else None,\n        metadata=NodeMetadata(\n            node_id=final_node_id,\n            node_type=\"script_chain\",\n            name=self.name,\n            version=\"1.0.0\",\n            start_time=start_time,\n            end_time=end_time,\n            duration=duration,\n        ),  # type: ignore[call-arg]\n        execution_time=duration,\n        token_stats=self.metrics.as_dict(),\n    )\n</code></pre>"},{"location":"api_reference/#ice_orchestrator.ScriptChain.execute_node","title":"<code>execute_node(node_id, input_data)</code>  <code>async</code>","text":"<p>Execute a single node using agent/tool wrappers (overrides BaseScriptChain).</p> Source code in <code>src/ice_orchestrator/script_chain.py</code> <pre><code>async def execute_node(self, node_id: str, input_data: Dict[str, Any]) -&gt; NodeExecutionResult:\n    \"\"\"Execute a single node using agent/tool wrappers (overrides BaseScriptChain).\"\"\"\n    node = self.nodes.get(node_id)\n    if node is None:\n        raise ValueError(f\"Node '{node_id}' not found in chain configuration\")\n\n    # ------------------------------------------------------------------\n    # Persist *input_data* to the context store ------------------------\n    # ------------------------------------------------------------------\n    self.context_manager.update_node_context(\n        node_id=node_id,\n        content=input_data,\n        execution_id=self.context_manager.get_context().execution_id  # type: ignore[attr-defined]\n    )\n\n    max_retries: int = int(getattr(node, \"retries\", 0))\n    base_backoff: float = float(getattr(node, \"backoff_seconds\", 0.0))\n\n    attempt = 0\n    last_error: Exception | None = None\n\n    while attempt &lt;= max_retries:\n        try:\n            # ------------------------------------------------------\n            # Cache lookup (opt-in) --------------------------------\n            # ------------------------------------------------------\n            import hashlib\n            import json\n\n            cache_key: str | None = None\n            if self.use_cache and getattr(node, \"use_cache\", True):\n                try:\n                    # Include *node configuration* snapshot so that changes\n                    # (e.g. modified prompt) bust the cache automatically.\n                    from pydantic import BaseModel\n\n                    cfg_payload = (\n                        node.model_dump() if isinstance(node, BaseModel) else str(node)\n                    )\n\n                    payload = {\n                        \"node_id\": node_id,\n                        \"input\": input_data,\n                        \"cfg\": cfg_payload,\n                    }\n                    serialized = json.dumps(payload, sort_keys=True, default=str)\n                    cache_key = hashlib.sha256(serialized.encode()).hexdigest()\n                    cached = self._cache.get(cache_key)\n                    if cached is not None:\n                        return cached\n                except Exception:\n                    # Fall back \u2013 never fail due to cache issues\n                    cache_key = None\n            # ----------------------------------------------------------\n            # Dispatch via the *Node Registry* -------------------------\n            # ----------------------------------------------------------\n            executor = get_executor(str(getattr(node, \"type\", \"\")))  # type: ignore[arg-type]\n            # ------------------------------------------------------\n            # Per-node tracing span --------------------------------\n            # ------------------------------------------------------\n            with tracer.start_as_current_span(\n                \"node.execute\",\n                attributes={\n                    \"node_id\": node_id,\n                    \"node_type\": str(getattr(node, \"type\", \"\")),\n                },\n            ) as node_span:\n                result = await executor(self, node, input_data)\n\n                # Attach outcome metadata to the span ---------------\n                node_span.set_attribute(\"success\", result.success)\n                node_span.set_attribute(\"retry_count\", attempt)\n                if not result.success:\n                    node_span.set_status(Status(StatusCode.ERROR, result.error or \"\"))\n\n            # Store in cache if enabled &amp; succeeded ------------------\n            if cache_key and result.success:\n                self._cache.set(cache_key, result)\n\n            # Attach retry metadata -----------------------------------\n            if result.metadata:\n                result.metadata.retry_count = attempt\n\n            # Persist *output* to the context store if configured ------\n            if self.persist_intermediate_outputs and result.output is not None:\n                self.context_manager.update_node_context(\n                    node_id=node_id,\n                    content=result.output,\n                    execution_id=self.context_manager.get_context().execution_id  # type: ignore[attr-defined]\n                )\n\n            # ----------------------------------------------------------\n            # Optional output validation -------------------------------\n            # ----------------------------------------------------------\n            if self.validate_outputs and getattr(node, \"output_schema\", None):\n                if not self._is_output_valid(node, result.output):\n                    result.success = False\n                    err_msg = (\n                        f\"Output validation failed for node '{node_id}' against declared schema\"\n                    )\n                    result.error = (\n                        err_msg if result.error is None else result.error + \"; \" + err_msg\n                    )\n\n            return result\n\n        except Exception as e:  # pylint: disable=broad-except\n            last_error = e\n            if attempt &gt;= max_retries:\n                break\n\n            # ------------------------------------------------------\n            # Exponential backoff before next retry ---------------\n            # ------------------------------------------------------\n            wait_seconds = base_backoff * (2 ** attempt) if base_backoff &gt; 0 else 0\n            if wait_seconds &gt; 0:\n                await asyncio.sleep(wait_seconds)\n\n            attempt += 1\n\n    # ------------------------------------------------------------------\n    # All retries exhausted \u2013 return failure result ---------------------\n    # ------------------------------------------------------------------\n    from datetime import datetime\n\n    error_meta = NodeMetadata(\n        node_id=node_id,\n        node_type=str(getattr(node, \"type\", \"\")),\n        name=getattr(node, \"name\", None),\n        version=\"1.0.0\",\n        start_time=datetime.utcnow(),\n        end_time=datetime.utcnow(),\n        duration=0.0,\n        error_type=type(last_error).__name__ if last_error else \"UnknownError\",\n        retry_count=attempt,\n    )  # type: ignore[call-arg]\n\n    if self.failure_policy == FailurePolicy.HALT:\n        raise last_error if last_error else Exception(\"Unknown error\")\n\n    return NodeExecutionResult(  # type: ignore[call-arg]\n        success=False,\n        error=f\"Retry limit exceeded ({max_retries}) \u2013 last error: {last_error}\",\n        metadata=error_meta,\n    )\n</code></pre>"},{"location":"api_reference/#ice_orchestrator.ScriptChain.get_level_nodes","title":"<code>get_level_nodes(level)</code>","text":"<p>Get nodes at a specific level.</p> Source code in <code>src/ice_orchestrator/script_chain.py</code> <pre><code>def get_level_nodes(self, level: int) -&gt; List[str]:\n    \"\"\"Get nodes at a specific level.\"\"\"\n    return self.levels.get(level, [])\n</code></pre>"},{"location":"api_reference/#ice_orchestrator.ScriptChain.get_metrics","title":"<code>get_metrics()</code>","text":"<p>Get execution metrics.</p> Source code in <code>src/ice_orchestrator/script_chain.py</code> <pre><code>def get_metrics(self) -&gt; Dict[str, Any]:\n    \"\"\"Get execution metrics.\"\"\"\n    return self.metrics.as_dict()\n</code></pre>"},{"location":"api_reference/#ice_orchestrator.ScriptChain.get_node_dependencies","title":"<code>get_node_dependencies(node_id)</code>","text":"<p>Get dependencies for a node.</p> Source code in <code>src/ice_orchestrator/script_chain.py</code> <pre><code>def get_node_dependencies(self, node_id: str) -&gt; List[str]:\n    \"\"\"Get dependencies for a node.\"\"\"\n    return self.graph.get_node_dependencies(node_id)\n</code></pre>"},{"location":"api_reference/#ice_orchestrator.ScriptChain.get_node_dependents","title":"<code>get_node_dependents(node_id)</code>","text":"<p>Get dependents for a node.</p> Source code in <code>src/ice_orchestrator/script_chain.py</code> <pre><code>def get_node_dependents(self, node_id: str) -&gt; List[str]:\n    \"\"\"Get dependents for a node.\"\"\"\n    return self.graph.get_node_dependents(node_id)\n</code></pre>"},{"location":"api_reference/#ice_orchestrator.ScriptChain.get_node_level","title":"<code>get_node_level(node_id)</code>","text":"<p>Get execution level for a node.</p> Source code in <code>src/ice_orchestrator/script_chain.py</code> <pre><code>def get_node_level(self, node_id: str) -&gt; int:\n    \"\"\"Get execution level for a node.\"\"\"\n    return self.graph.get_node_level(node_id)\n</code></pre>"},{"location":"api_reference/#ice_orchestrator.WorkflowExecutionContext","title":"<code>WorkflowExecutionContext</code>","text":"<p>Holds workflow-wide execution settings, output format requirements, and preferences for ScriptChain orchestration.</p> Source code in <code>src/ice_orchestrator/workflow_execution_context.py</code> <pre><code>class WorkflowExecutionContext:\n    \"\"\"\n    Holds workflow-wide execution settings, output format requirements, and preferences for ScriptChain orchestration.\n    \"\"\"\n\n    def __init__(\n        self,\n        mode: str = \"auto\",\n        require_json_output: bool = False,\n        strict_validation: bool = False,\n        user_preferences: Optional[Dict[str, Any]] = None,\n        **kwargs,\n    ):\n        self.mode = mode  # e.g., 'tool-calling', 'chat', 'summarization', etc.\n        self.require_json_output = require_json_output\n        self.strict_validation = strict_validation\n        self.user_preferences = user_preferences or {}\n        # Store any additional context fields\n        for k, v in kwargs.items():\n            setattr(self, k, v)\n\n    def as_dict(self) -&gt; Dict[str, Any]:\n        return {\n            \"mode\": self.mode,\n            \"require_json_output\": self.require_json_output,\n            \"strict_validation\": self.strict_validation,\n            \"user_preferences\": self.user_preferences,\n            **{\n                k: v\n                for k, v in self.__dict__.items()\n                if k\n                not in {\n                    \"mode\",\n                    \"require_json_output\",\n                    \"strict_validation\",\n                    \"user_preferences\",\n                }\n            },\n        }\n</code></pre>"},{"location":"architecture/","title":"Architecture","text":"<pre><code>graph TD\n    subgraph Web Layer\n        A[FastAPI App]\n        A --&gt; B[Core Routers]\n        A --&gt; C[KB Router]\n    end\n\n    subgraph Orchestrator\n        D[ScriptChain]\n        D --&gt; E[NODE Executors]\n    end\n\n    subgraph SDK\n        F[Node &amp; Agent Models]\n        F --&gt; G[ToolService]\n        F --&gt; H[GraphContextManager]\n    end\n\n    subgraph CLI\n        I[Typer-based CLI]\n    end\n\n    A --&gt; D\n    D --&gt; F\n    F --&gt; I\n</code></pre> <p>High-level layers:</p> Layer Purpose FastAPI App Exposes REST + real-time endpoints for orchestrator, chain-builder and knowledge-base. Orchestrator Executes DAGs with concurrency, caching and metrics. SDK Data models, executors, context and tool abstractions. CLI Local developer ergonomics (chain builder, tool runner, server launcher). <p>Dive deeper into each component in the pages that follow. </p>"},{"location":"cli_reference/","title":"CLI Reference","text":"<p>The CLI is built with Typer.  Generate a fresh reference with:</p> <pre><code>ice --help &gt; docs/cli_reference.md\n</code></pre> <p>Current commands snapshot (partial):</p> <pre><code>Usage: ice [OPTIONS] COMMAND [ARGS]...\n\niceOS developer CLI\n\nOptions:\n  --help  Show this message and exit.\n\nCommands:\n  init  Initialise an .ice workspace and developer environment\n  ls    List tools (shortcut for 'tool ls')\n  run   Execute a ScriptChain declared in a Python file\n  sdk   Opinionated scaffolds for tools, nodes and chains\n  tool  Commands related to tool development\n</code></pre>"},{"location":"contrib/","title":"Contrib &amp; Extensions","text":"<ul> <li><code>ice_sdk_contrib</code> contains optional helpers that extend the core SDK.</li> <li>Community members can publish their own <code>ice_sdk_&lt;name&gt;</code> PyPI packages and follow the same import pattern. </li> </ul>"},{"location":"deployment/","title":"Deployment &amp; Ops","text":"<p>Topics to be covered: * Docker images &amp; multi-stage build * Health checks (<code>/health</code>) * Make targets (<code>make doctor</code>, <code>make prod</code>) * Observability via OTEL * Continuous deployment of docs via <code>mkdocs gh-deploy --force</code> in CI pipeline. </p>"},{"location":"docker_poetry_strategy/","title":"Docker poetry strategy","text":"<p>d# iceOS Deployment &amp; Packaging Strategy</p> <p>Revision: 2025-XX-XX</p> <p>This document explains how we use Poetry and Docker today, the options we have for packaging &amp; deploying the platform tomorrow, and step-by-step check-lists so we can move smoothly from local hacking to a fully containerised, micro-service architecture.</p>"},{"location":"docker_poetry_strategy/#1-current-state","title":"1\u2003Current state","text":"Layer Tooling Purpose Where we use it Python dependency + virtual-env Poetry (<code>pyproject.toml</code> + <code>poetry.lock</code>) \u2022 Pin exact versions\u2022 Create per-project virtual-env Local dev, CI System-level isolation Docker (pulled via <code>testcontainers</code>) Spin up throw-away services for tests (httpbin, Postgres, \u2026) CI + contract/integration tests only <p>There is no application Docker image yet; the platform is started locally with:</p> <pre><code>poetry install\npoetry run uvicorn app.main:app --reload\n</code></pre>"},{"location":"docker_poetry_strategy/#2-why-we-keep-both-tools","title":"2\u2003Why we keep both tools","text":"<p>\u2022 Poetry = repeatable Python layer   \u2013 Guarantees every dev/CI host has the same package set.   \u2013 Handles publishing to PyPI for power users who want <code>pip install iceos</code>.</p> <p>\u2022 Docker = repeatable OS/process layer   \u2013 Freezes OS libs (libssl, libc, \u2026) &amp; shell tools.   \u2013 Lets us scale / isolate services in production or spin up side-car services in tests.</p> <p>Using both means we can: 1. Develop quickly (no Docker build loop). 2. Ship a one-liner to self-hosters (<code>docker run \u2026</code>). 3. Later break the monolith into multiple containers without changing dev workflow.</p>"},{"location":"docker_poetry_strategy/#3-roadmap-of-packaging-modes","title":"3\u2003Roadmap of packaging modes","text":"Milestone Target audience Packaging Status M0 \u2013 Local dev &amp; CI Core team Poetry virtual-env \u2705 (today) M1 \u2013 Single-container release Self-hosters, staging One Docker image with Poetry inside \ud83d\udd1c M2 \u2013 Multi-container deployment SaaS / prod scale Compose / Kubernetes (UI, Orchestrator, Nodes) Future M3 \u2013 Language-agnostic Nodes Plugin authors Per-node images (Python, JS, JVM, etc.) Future"},{"location":"docker_poetry_strategy/#4-m1-one-image-recipe","title":"4\u2003M1  \u2022  One-image recipe","text":"<p>Create <code>Dockerfile</code> at repo root:</p> <pre><code>FROM python:3.11-slim\n\n# \u2011- Poetry -----------------------------------------------------------------\nENV POETRY_VERSION=1.8.3 \\\n    POETRY_HOME=/opt/poetry \\\n    POETRY_NO_INTERACTION=1 \\\n    PYTHONDONTWRITEBYTECODE=1 \\\n    PIP_DISABLE_PIP_VERSION_CHECK=1\nRUN pip install \"poetry==$POETRY_VERSION\"\nENV PATH=\"${POETRY_HOME}/bin:${PATH}\"\n\n# \u2011- Install deps first (cached layer) --------------------------------------\nWORKDIR /app\nCOPY pyproject.toml poetry.lock /app/\nRUN poetry install --no-root --only main\n\n# \u2011- Copy source -------------------------------------------------------------\nCOPY src /app/src\n\n# \u2011- Entrypoint --------------------------------------------------------------\nEXPOSE 8000\nCMD [\"poetry\", \"run\", \"uvicorn\", \"app.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n</code></pre> <p>Build &amp; test locally:</p> <pre><code>docker build -t iceos:latest .\ndocker run -p 8000:8000 iceos:latest\n</code></pre> <p>CI addition (<code>.github/workflows/ci.yml</code> snippet):</p> <pre><code>      - name: Build app image\n        run: docker build -t ghcr.io/iceos/iceos:${{ github.sha }} .\n</code></pre> <p>Publish with GitHub Actions or push manually.</p>"},{"location":"docker_poetry_strategy/#5-m2-breaking-into-services","title":"5\u2003M2  \u2022  Breaking into services","text":"<p>When traffic or team velocity warrants, split into independent images:</p> Service Image name Port Notes Public REST / UI <code>iceos/web</code> 80 Next-JS or FastAPI \"canvas\" front-end Orchestrator API <code>iceos/orchestrator</code> 9000 Schedules chains, manages events Built-in Nodes <code>iceos/nodes-std</code> \u2014 Package of light Python tools Heavy Nodes (GPU/Large models) <code>iceos/node-llm</code> \u2014 Can scale separately Postgres / Redis upstream images \u2014 state &amp; queues <p>Example <code>docker-compose.yml</code> skeleton:</p> <pre><code>version: \"3.9\"\nservices:\n  web:\n    image: iceos/web:${TAG}\n    depends_on: [orchestrator]\n    ports: [\"80:80\"]\n  orchestrator:\n    image: iceos/orchestrator:${TAG}\n    environment:\n      DB_URL: postgres://...\n    depends_on: [db]\n  db:\n    image: postgres:16\n    volumes: [db-data:/var/lib/postgresql/data]\nvolumes:\n  db-data:\n</code></pre> <p>Later swap Compose for Helm charts if you move to Kubernetes.</p>"},{"location":"docker_poetry_strategy/#6-guidelines-for-node-authors","title":"6\u2003Guidelines for Node authors","text":"<ol> <li>Side-effect isolation \u2013 All external effects live only inside <code>Tool.run()</code> implementations (repo rule #2).  This makes it trivial to ship each Tool as its own container/API.</li> <li>Async I/O \u2013 Keep <code>run()</code> async to avoid blocking orchestrator threads when nodes are co-located.</li> <li>Environment contracts \u2013 Node images expose:</li> <li>Health endpoint at <code>/healthz</code> (HTTP 200 = OK)</li> <li>Metadata at <code>/openapi.json</code> so orchestrator can discover inputs/outputs.</li> <li>Versioning \u2013 Bump image tags with semantic versioning; orchestrator chooses compat image based on chain requirements.</li> </ol>"},{"location":"docker_poetry_strategy/#7-when-to-move-to-the-next-milestone","title":"7\u2003When to move to the next milestone","text":"Symptom Consider\u2026 You want one-liner install for beta testers M1 (single image) CPU spikes in Orchestrator starve UI M2 (split UI / orchestrator) Large-model node needs GPU M2/M3 (dedicated node image on GPU pool) External contributors publish their own nodes M3 (per-node images, registry)"},{"location":"docker_poetry_strategy/#8-reference-commands-cheat-sheet","title":"8\u2003Reference commands cheat-sheet","text":"<pre><code># Local dev\npoetry install              # once\npoetry run uvicorn app.main:app --reload\n\n# Run full tests (uses Docker for contract tests)\nmake test                   # or poetry run pytest -m \"not slow\"\n\n# Build single-image release\nTAG=$(git rev-parse --short HEAD)\ndocker build -t iceos:${TAG} .\n\ndocker run -p 8000:8000 iceos:${TAG}\n\n# Compose stack (once images exist)\ndocker compose up -d\n</code></pre>"},{"location":"docker_poetry_strategy/#9-appendix-glossary","title":"9\u2003Appendix \u2013 glossary","text":"Term Meaning Poetry Python dependency &amp; packaging tool; replaces <code>pip + virtualenv + setup.py</code> Docker image Read-only template (like a OS snapshot) used to start containers Container Running instance of an image, isolated by the host kernel Monolith One process/image containing whole app Micro-service Many small images/processes communicating over APIs Testcontainers Library that spins up Docker containers during tests"},{"location":"docker_poetry_strategy/#end-of-document","title":"End of document","text":""},{"location":"healthchecks/","title":"Health-check Matrix","text":"<p>Run <code>make doctor</code> (wrapper for <code>python -m scripts.doctor</code>) to execute the full suite. Each row below is also runnable standalone.</p> # Check Command Expected Output 1 Linting (ruff) <code>ruff check src/</code> No violations 2 Typing (pyright) <code>pyright</code> 0 errors 3 Unit &amp; integration tests <code>make test</code> All tests pass 4 Coverage threshold <code>pytest --cov=ice_sdk --cov=ice_orchestrator --cov-fail-under=65</code> \u2265 65 % coverage 5 Security audit <code>pip-audit</code> 0 vulnerabilities 6 Import-linter rules <code>python -m importlinter</code> All contracts green 7 isort check <code>isort --check-only src/</code> Passed 8 JSON/YAML validity <code>python -m scripts.check_json_yaml</code> 0 invalid files 9 Schema generation drift <code>python -m scripts.generate_schemas &amp;&amp; git diff --exit-code schemas/runtime</code> No diff 10 Doc build freshness <code>make refresh-docs &amp;&amp; git diff --exit-code docs/</code> No uncontrolled diff 11 \u23f1\ufe0f Perf smoke <code>python -m scripts.doctor --perf</code> &lt; 2 s per suite 12 License headers <code>python -m scripts.check_license</code> Headers present"},{"location":"next_steps_chain_wizard/","title":"Interactive Chain Builder \u2013 Implementation Roadmap","text":"<p>Status: In Progress (target release <code>v0.2.0</code>)</p> <p>This document captures the design blueprint and incremental milestones for the <code>ice sdk create-chain --builder</code> feature.</p>"},{"location":"next_steps_chain_wizard/#1-architecture-layers","title":"1  Architecture Layers","text":"Layer Responsibility Notes IO layer Renders questions / collects answers (CLI prompts today, UI or agent later) Typer + Questionary for v0.  Copilot will swap this layer. Builder-Engine Stateless state-machine that emits Question \u2192 Answer cycles and finally a <code>ChainDraft</code> Lives in <code>ice_cli.chain_builder.engine</code> (pure Python, no UI deps). Template layer Converts <code>ChainDraft</code> \u2192 concrete <code>NodeConfig</code> objects + <code>ScriptChain</code> Python scaffold Re-uses existing template helpers."},{"location":"next_steps_chain_wizard/#2-question-catalogue-v0","title":"2  Question Catalogue (v0)","text":"<ol> <li>Chain meta    \u2022 name    \u2022 persist_interm_outputs (y/N)</li> <li>Node loop (repeats)    \u2022 node_type (ai / tool / agent)    \u2022 node_name / id    \u2022 dependencies (multi-select)    \u2022 type-specific extras (model &amp; prompt, tool ref, agent instructions \u2026)    \u2022 advanced (retries, timeout, cache)</li> <li>Review    \u2022 Render Mermaid graph + summary table    \u2022 Confirm \u2192 write files / run / exit</li> </ol>"},{"location":"next_steps_chain_wizard/#3-milestones","title":"3  Milestones","text":"Milestone Scope ETA M0 Linear graphs, <code>ai</code> + <code>tool</code> nodes, up to 10 nodes, writes <code>.chain.py</code>. 1 week M1 Mermaid graph preview, validation (dup IDs, cycles). +3 days M2 Builder-Engine REST fa\u00e7ade for Copilot, CLI remains a thin client. +1 week M3 Branching, <code>condition</code> &amp; <code>sink</code> nodes, import existing YAMLs. +2 weeks"},{"location":"next_steps_chain_wizard/#4-copilot-integration-ideas","title":"4  Copilot Integration Ideas","text":"<ul> <li>Copilot feeds answers programmatically via REST, enabling natural-language flow authoring.</li> <li>Live cost estimation + policy checks surfaced during the Q&amp;A loop.</li> </ul>"},{"location":"next_steps_chain_wizard/#5-technical-notes","title":"5  Technical Notes","text":"<ul> <li>Draft state stored in memory (JSON), serialisable to <code>draft_chain.yaml</code>.</li> <li>Node IDs auto-slugify names + numeric suffix.</li> <li>Engine unit-tested independent of UI via scripted answer sequences.</li> </ul> <p>Last updated: {{ build_date }} by <code>Cursor AI</code> </p>"},{"location":"quick_start/","title":"Quick Start","text":"<pre><code># 1. Clone &amp; install\npoetry install --with dev\n\n# 2. Run unit tests &amp; quality gate\nmake doctor\n\n# 3. Launch FastAPI server (auto-reload)\nmake dev\n\n# 4. Call the health-check\ncurl http://localhost:8000/health\n\n# 5. Run a trivial chain via CLI\nice chain run examples/echo_chain.json\n</code></pre> <p>For a deeper dive into ScriptChain and node configuration see the Core Concepts section. </p>"},{"location":"core_concepts/nodes_tools_agents/","title":"Nodes, Tools &amp; Agents","text":""},{"location":"core_concepts/nodes_tools_agents/#nodes","title":"Nodes","text":"<ul> <li>AiNodeConfig \u2013 LLM-backed step with prompt, mappings and output schema.</li> <li>ToolNodeConfig \u2013 deterministic call to a registered tool.</li> <li>ConditionNodeConfig \u2013 branching logic based on context expression.</li> </ul> <p>All nodes share common fields (id, dependencies, retries, timeout, etc.) defined in <code>BaseNodeConfig</code>.</p>"},{"location":"core_concepts/nodes_tools_agents/#tools","title":"Tools","text":"<p>Implementation contract:</p> <pre><code>class MyTool(BaseTool):\n    name = \"my_tool\"\n    description = \"Does something deterministic\"\n    parameters_schema = {...}\n\n    async def run(self, **kwargs):\n        ...\n</code></pre> <p>Tools must be side-effect free except inside <code>run</code> (repo rule #2) and declare a JSONSchema for parameters so LLMs can plan calls.</p>"},{"location":"core_concepts/nodes_tools_agents/#agents","title":"Agents","text":"<p>An <code>AgentNode</code> embeds an LLM planner + limited tool set and can expose itself as a tool for recursive planning.  Cycle detection is enforced with a context-local stack to prevent infinite loops. </p>"},{"location":"core_concepts/script_chain/","title":"ScriptChain Execution Model","text":"<p>ScriptChain executes a directed-acyclic graph in levels (topological depth).  Nodes on the same level run concurrently up to <code>max_parallel</code>.</p> <p>Key features:</p> Feature Location Notes Failure policies <code>FailurePolicy</code> enum in <code>base_script_chain.py</code> <code>CONTINUE_POSSIBLE</code>, <code>HALT_ON_ERROR</code>, etc. Caching <code>ice_sdk.cache</code> Hashes node inputs + config Guards <code>token_guard</code>, <code>depth_guard</code> callbacks Abort politely when ceilings exceeded Metrics <code>ChainMetrics</code> Aggregates token / cost per node Observability OpenTelemetry spans Integrate with Jaeger / Honeycomb <p>Sequence diagram coming soon.</p> <pre><code>sequenceDiagram\n    participant C as Client\n    participant SC as ScriptChain\n    participant N1 as Node A\n    participant N2 as Node B\n    participant N3 as Node C\n\n    C-&gt;&gt;SC: execute()\n    SC-&gt;&gt;N1: run()\n    SC--&gt;&gt;N1: await\n    SC-&gt;&gt;N2: run()\n    SC--&gt;&gt;N2: await\n    SC-&gt;&gt;N3: run()\n    N3--&gt;&gt;SC: result\n    SC--&gt;&gt;C: ChainExecutionResult\n</code></pre>"},{"location":"demos/frosty_personal_brand_demo/","title":"Frosty Demo \u2013 Taylor's Personal Brand Growth System","text":"<p>Demo walkthrough for iceOS v0.x</p>"},{"location":"demos/frosty_personal_brand_demo/#1-why-this-demo","title":"1  Why This Demo?","text":"<p>Taylor \u2013 a 26-year-old marketing freelancer \u2013 wants to 5\u00d7 her content output without losing her authentic voice.  The demo shows how iceOS orchestrates LLM nodes and deterministic tools to automate Taylor's content workflow today (manual chain) and paves the way for the future Frosty platform agent.</p> <p>Outcome Targets (90 days) \u2022 Content output \u2192 3\u200a/\u200awk \u2192 5-7\u200a/\u200aday \u2022 Engagement rate \u2192 4.2 % \u2192 8.5 % \u2022 Leads \u2192 2-3\u200a/\u200awk \u2192 10-15\u200a/\u200awk</p>"},{"location":"demos/frosty_personal_brand_demo/#2-architecture-at-a-glance","title":"2  Architecture at a Glance","text":"<pre><code>graph TD\n  subgraph Knowledge\n    KB[Knowledge Base]\n  end\n  subgraph Content_Engine\n    AIDEA[IdeaGenerator (ai)] --&gt;|voice| APPLIER[VoiceApplier (tool)]\n    APPLIER --&gt; INJECT[TestimonialInjector (tool)]\n    INJECT --&gt; LEN{Length Check (condition)}\n    LEN -- pass --&gt; FORMAT[FormatOptimizer (tool)]\n    LEN -- fail --&gt; REWRITE[Rewriter (ai)]\n    FORMAT --&gt; SPLIT[PlatformSplitter (tool)]\n  end\n  subgraph Channels\n    SPLIT --&gt; TW[TwitterFormatter (tool)] --&gt; POST_TW[TwitterPoster (tool)]\n    SPLIT --&gt; LI[LinkedInFormatter (tool)] --&gt; POST_LI[LinkedInPoster (tool)]\n  end\n  POST_TW &amp; POST_LI --&gt; TRACKER[PerformanceTracker (tool)] --&gt; OPT[OptimizationAgent (ai)] --&gt; KB\n</code></pre> <p>Each rectangle is a Node executed by the async <code>ScriptChain</code> engine: - <code>(ai)</code> \u2192 <code>AiNode</code> powered by an LLM provider. - <code>(tool)</code> \u2192 <code>ToolNode</code> with deterministic side-effects. - <code>(condition)</code> \u2192 <code>ConditionNode</code> for branching.</p>"},{"location":"demos/frosty_personal_brand_demo/#3-knowledge-base-layout","title":"3  Knowledge Base Layout","text":"<pre><code>/knowledge_base\n\u251c\u2500\u2500 voice_rules.json           # stylistic replacements\n\u251c\u2500\u2500 testimonials.csv           # \"quote\",\"topic\"\n\u251c\u2500\u2500 top_content/*.json         # prior high-performers \u2013 vector-indexed\n\u2514\u2500\u2500 audience.parquet           # demographic &amp; pain-point data\n</code></pre> <p>Persist it anywhere (local path or S3) \u2013 the tools consume absolute paths.</p>"},{"location":"demos/frosty_personal_brand_demo/#4-node-configuration-snippets","title":"4  Node Configuration Snippets","text":"<p>Below are ready-to-run Pydantic models (Python dict / JSON) that follow the public SDK.  IDs are UUID-v4 strings shortened for readability.</p>"},{"location":"demos/frosty_personal_brand_demo/#41-ideagenerator-ainode","title":"4.1 IdeaGenerator (AiNode)","text":"<pre><code>{\n  \"id\": \"node-idea-1\",\n  \"type\": \"ai\",\n  \"name\": \"Idea Generator\",\n  \"dependencies\": [],\n  \"model\": \"gpt-4o-mini\",\n  \"prompt\": \"SYSTEM:\\nYou are an AI copywriter for Taylor (26-year-old marketing freelancer).\\nMaintain first-person, energetic voice and end every idea with an actionable hook in &lt;=15 words.\\nReturn a JSON list called ideas containing exactly 5 strings.\\n---\\nUSER_INPUT: {{topic}}\\n\",\n  \"llm_config\": {\n    \"provider\": \"openai\",\n    \"model\": \"gpt-4o-mini\",\n    \"temperature\": 0.8,\n    \"max_tokens\": 300\n  },\n  \"metadata\": {\n    \"version\": \"1.0.0\"\n  }\n}\n</code></pre>"},{"location":"demos/frosty_personal_brand_demo/#42-voiceapplier-toolnode","title":"4.2 VoiceApplier (ToolNode)","text":"<pre><code>{\n  \"id\": \"node-voice-1\",\n  \"type\": \"tool\",\n  \"tool_name\": \"VoiceApplier\",\n  \"dependencies\": [\"node-idea-1\"],\n  \"tool_args\": {\n    \"rules_path\": \"./knowledge_base/voice_rules.json\"\n  },\n  \"metadata\": {\n    \"version\": \"1.0.0\"\n  }\n}\n</code></pre>"},{"location":"demos/frosty_personal_brand_demo/#43-length-check-conditionnode","title":"4.3 Length Check (ConditionNode)","text":"<pre><code>{\n  \"id\": \"node-len-1\",\n  \"type\": \"condition\",\n  \"dependencies\": [\"node-testimonial-1\"],\n  \"expression\": \"len(context['text']) &lt;= 280\",\n  \"true_branch\": [\"node-format-1\"],\n  \"false_branch\": [\"node-rewrite-1\"],\n  \"metadata\": {\n    \"version\": \"1.0.0\"\n  }\n}\n</code></pre> <p>Tip: any JMESPath-style expression is fine \u2013 the orchestrator only evaluates the boolean.</p> <p>Add the remaining formatter and poster nodes analogously.</p>"},{"location":"demos/frosty_personal_brand_demo/#5-running-the-demo","title":"5  Running the Demo","text":""},{"location":"demos/frosty_personal_brand_demo/#51-prerequisites","title":"5.1 Prerequisites","text":"<ol> <li><code>git clone</code> this repo and <code>cd iceOSv1-A-</code></li> <li><code>python -m venv .venv &amp;&amp; source .venv/bin/activate</code></li> <li><code>pip install -e .[dev]</code></li> <li>Export at least one LLM key: <code>bash    export OPENAI_API_KEY=\"sk-...\"</code></li> <li>Place the <code>knowledge_base/</code> folder at project root (or update tool args).</li> </ol>"},{"location":"demos/frosty_personal_brand_demo/#52-execute-via-scriptchain","title":"5.2 Execute via <code>ScriptChain</code>","text":"<pre><code>import asyncio, json\nfrom ice_orchestrator import ScriptChain\nfrom pathlib import Path\n\nnodes = json.loads(Path(\"demo_nodes.json\").read_text())\n\nasync def main():\n    chain = ScriptChain(nodes=nodes, name=\"taylor_demo\")\n    result = await chain.execute()\n    print(result.model_dump())\n\nasyncio.run(main())\n</code></pre>"},{"location":"demos/frosty_personal_brand_demo/#53-execute-via-rest-api","title":"5.3 Execute via REST API","text":"<pre><code>uvicorn app.main:app --reload &amp;  # local dev server\n\ncurl -X POST http://localhost:8000/api/v1/workflow \\\n  -H \"Content-Type: application/json\" \\\n  -d @demo_request.json | jq\n</code></pre> <p>The JSON payload mirrors the <code>WorkflowRequest</code> model (list of NodeConfigs + optional initial context).</p>"},{"location":"demos/frosty_personal_brand_demo/#6-observing-the-run","title":"6  Observing the Run","text":"<p><code>ScriptChain</code> logs each level in parallel.  Check: - <code>logs/</code> folder or stdout for timing &amp; retries. - Context cache via <code>GET /api/v1/nodes/{node_id}/context</code>.</p> <pre><code>\u2705  node-idea-1  0.9s\n\u2705  node-voice-1  0.1s  (cached)\n\u274c  node-rewrite-1  retry 1/2 \u2026\n</code></pre>"},{"location":"demos/frosty_personal_brand_demo/#7-extending-towards-frosty","title":"7  Extending Towards \"Frosty\"","text":"<p>The future Frosty Agent will wrap this chain behind a conversational interface, but every component above already complies with iceOS rules: 1. Type-Safe \u2013 all configs are <code>NodeConfig</code> sub-classes. 2. Side-Effects in Tools \u2013 posters &amp; DB writes are <code>ToolNode</code>s. 3. Async \u2013 long I/O (HTTP, DB) is <code>await</code>-ed inside tool <code>execute()</code> methods. 4. Event Naming \u2013 emitted events follow <code>demoAgent.eventVerb</code> (e.g. <code>demoAgent.postScheduled</code>).</p>"},{"location":"demos/frosty_personal_brand_demo/#8-next-steps-for-contributors","title":"8  Next Steps for Contributors","text":"<ol> <li><code>ice sdk create-node VoiceApplier</code> \u2013 scaffold with Pydantic schema &amp; unit test.  </li> <li>Write unit tests under <code>tests/tools/test_voice_applier.py</code> and run <code>make test</code>.  </li> <li>Update docs here and open a PR \u2013 CI will enforce Ruff, MyPy &amp; import-linter contracts.</li> </ol> <p>Made with \u2603\ufe0f &amp; async love. </p>"},{"location":"how_to/agent/","title":"How-to: Wrap an API as an Agent","text":"<p>In this tutorial we wrap the public DuckDuckGo Instant-Answer API with an AgentNode so it can chain tool calls, parse JSON, and provide a concise reply.</p>"},{"location":"how_to/agent/#1-create-the-agentconfig","title":"1  Create the AgentConfig","text":"<p>```python title=\"agents/duck_agent.py\" from ice_sdk.models.agent_models import AgentConfig, ModelSettings from ice_sdk.tools.web_search import WebSearchTool</p> <p>DuckAgent = AgentConfig(     name=\"duck_agent\",     instructions=\"\"\" You are a helpful assistant that answers factual questions using DuckDuckGo. If you are unsure, respond with \"I don't know\". \"\"\".strip(),     model_settings=ModelSettings(provider=\"openai\", model=\"gpt-3.5-turbo\"),     tools=[WebSearchTool()] )</p> <pre><code>\nNotes:\n* Tools list is **whitelist** \u2013 the LLM can only call those.\n* `max_rounds` defaults to 3; increase if you expect multiple tool calls.\n\nPlace the file anywhere under `src/` so auto-discovery picks it up.\n\n## 2  Invoke via ScriptChain\n\n```yaml title=\"chains/duck_chain.yaml\"\n- id: ask_duck\n  type: ai\n  name: AskDuck\n  model: gpt-3.5-turbo\n  prompt: |\n    {{input.question}}\n  llm_config:\n    provider: openai\n    max_tokens: 512\n  tools:\n    - name: duck_agent  # exposed automatically as a tool\n  input_schema:\n    type: object\n    properties:\n      question:\n        type: string\n    required: [question]\n</code></pre>"},{"location":"how_to/agent/#3-run-the-chain","title":"3  Run the chain","text":"<pre><code>ice chain run chains/duck_chain.yaml -a '{\"question\": \"When was Apollo 11 launched?\"}'\n</code></pre> <p>The agent will decide whether to call <code>duck_agent</code> (which itself may call <code>web_search</code>).  Results propagate back to the chain.</p>"},{"location":"how_to/agent/#4-expose-via-http","title":"4  Expose via HTTP","text":"<p>Because the FastAPI server already exposes <code>/api/v1/chain/run</code>, the same YAML can be executed remotely:</p> <pre><code>curl -X POST http://localhost:8000/api/v1/chain/run \\\n  -d @chains/duck_chain.yaml \\\n  -H 'Content-Type: application/x-yaml'\n</code></pre>"},{"location":"how_to/custom_tool/","title":"How-to: Build a Custom Tool","text":"<p>This guide walks through creating WeatherTool, a deterministic wrapper around the public <code>wttr.in</code> HTTP endpoint.  We will:</p> <ol> <li>Scaffold the tool file.</li> <li>Implement the business logic with proper async IO.</li> <li>Declare the JSON parameters schema.</li> <li>Register &amp; test the tool.</li> </ol>"},{"location":"how_to/custom_tool/#1-scaffold","title":"1  Scaffold","text":"<p>Create <code>weather.tool.py</code> under any package (e.g. <code>user_tools/</code>):</p> <pre><code>mkdir -p src/user_tools\n$EDITOR src/user_tools/weather.tool.py\n</code></pre>"},{"location":"how_to/custom_tool/#2-implementation","title":"2  Implementation","text":"<p>```python title=\"src/user_tools/weather.tool.py\" from future import annotations</p> <p>import httpx from ice_sdk.tools.base import BaseTool</p> <p>class WeatherTool(BaseTool):     \"\"\"Return the current weather for a given city using wttr.in.\"\"\"</p> <pre><code>name = \"weather\"\ndescription = \"Fetch current weather report for a city (text format)\"\n\nparameters_schema = {\n    \"type\": \"object\",\n    \"properties\": {\n        \"city\": {\n            \"type\": \"string\",\n            \"description\": \"Target city, e.g. 'Berlin'\",\n        }\n    },\n    \"required\": [\"city\"],\n}\n\nasync def run(self, *, city: str, **_kwargs):  # type: ignore[override]\n    url = f\"https://wttr.in/{city}?format=3\"\n    async with httpx.AsyncClient() as client:\n        resp = await client.get(url, timeout=10)\n        resp.raise_for_status()\n        return {\"weather\": resp.text.strip()}\n</code></pre> <pre><code>\nNotes:\n* **Type hints** \u2013 mandatory by repo rule #1.\n* **No blocking IO** \u2013 HTTP call uses `httpx.AsyncClient` (rule #5).\n* **External side-effects** live only in `run` (rule #2).\n\n## 3  Auto-register\n\nThe orchestrator calls `ToolService.discover_and_register(project_root)` during app startup, which imports all `*.tool.py` modules under `src/`.  No further action needed.\n\n## 4  Test the tool\n\n```bash\n# CLI smoke test\nice tool test weather -a '{\"city\": \"Berlin\"}'\n\n# Within a ScriptChain (YAML excerpt)\n- id: weather_step\n  type: tool\n  tool_name: weather\n  tool_args:\n    city: Paris\n</code></pre> <p>You now have a reusable deterministic Tool accessible from Agents, ScriptChains and the HTTP API. </p>"}]}