"""Execution management REST endpoints."""

from __future__ import annotations

import asyncio
import datetime as _dt
import os
import uuid
from typing import Any, Dict, List, Optional
import sqlalchemy as sa

from fastapi import APIRouter, Body, Depends, HTTPException, Query, Request, status
from pydantic import BaseModel, Field

from ice_api.db.database_session_async import session_scope
from ice_api.db.database_session_async import get_session as _get_db_session
from ice_api.db.orm_models_core import BlueprintRecord as _BPRec
from ice_api.db.orm_models_core import ExecutionRecord, ExecutionEventRecord
from ice_api.redis_client import get_redis
from ice_core.models.mcp import Blueprint


# Duplicate event names locally to avoid appâ†’orchestrator import
class _Evt:
    NODE_STARTED = "node.started"
    NODE_COMPLETED = "node.completed"
    NODE_FAILED = "node.failed"
    WORKFLOW_STARTED = "workflow.started"
    WORKFLOW_COMPLETED = "workflow.completed"


router = APIRouter(prefix="/api/v1/executions", tags=["executions"])


class ExecutionStartResponse(BaseModel):
    """Response for starting a workflow execution.

    Args:
        execution_id (str): Identifier for the created execution.
        status (str): Current status (accepted, running, completed, failed, timeout).
        result (Dict[str, Any] | None): Final result when completed.

    Returns:
        ExecutionStartResponse: Response model containing execution details.
    """

    execution_id: str
    status: str
    result: Optional[Dict[str, Any]] = None


class ExecutionStatusResponse(BaseModel):
    """Response model for execution status."""

    execution_id: str
    status: str
    blueprint_id: Optional[str] = None
    result: Optional[Dict[str, Any]] = None
    error: Optional[str] = None
    events: Optional[List[Dict[str, Any]]] = None


class ExecutionsListItem(BaseModel):
    execution_id: str
    status: str
    blueprint_id: str


class ExecutionsListResponse(BaseModel):
    executions: List[ExecutionsListItem]


class ExecutionStartRequest(BaseModel):
    """Request body to start a workflow execution.

    Args:
        blueprint_id (str): Identifier of a previously stored blueprint.
        inputs (Dict[str, Any] | None): Optional initial inputs passed to the workflow.

    Returns:
        None
    """

    blueprint_id: str = Field(..., description="Stored blueprint id")
    inputs: Optional[Dict[str, Any]] = Field(
        default=None, description="Optional initial inputs for the workflow"
    )


from typing import TypedDict, cast

from ice_api.dependencies import rate_limit
from ice_api.security import require_auth


class _ExecutionRecord(TypedDict, total=False):
    status: str
    blueprint_id: str
    result: Any
    error: str
    _event: asyncio.Event
    events: list[Dict[str, Any]]


# ---------------------------------------------------------------------------
# Helpers
# ---------------------------------------------------------------------------


def _exec_key(execution_id: str) -> str:  # noqa: D401
    return f"exec:{execution_id}"


def _get_exec_store(request: Request) -> Dict[str, _ExecutionRecord]:  # noqa: D401
    # Keep in-memory store for in-process notifications; persist authoritative
    # state in Redis so runs survive restarts.
    if not hasattr(request.app.state, "executions"):
        request.app.state.executions = {}
    return cast(Dict[str, _ExecutionRecord], request.app.state.executions)  # type: ignore[attr-defined]


async def _get_blueprint(request: Request, blueprint_id: str) -> Blueprint:
    # In-process store (tests/dev)
    store = getattr(request.app.state, "blueprints", None)
    if store is not None and blueprint_id in store:
        return cast(Blueprint, store[blueprint_id])
    # DB authoritative
    try:
        async for session in _get_db_session():
            rec = await session.get(_BPRec, blueprint_id)
            if rec is not None:
                return Blueprint.model_validate(rec.body)
    except Exception:
        pass
    # Redis cache (best-effort)
    try:
        redis = get_redis()
        raw_json = await redis.hget(f"bp:{blueprint_id}", "json")  # type: ignore[arg-type]
        if raw_json:
            return Blueprint.model_validate_json(raw_json)
    except Exception:
        pass
    raise HTTPException(status_code=404, detail="Blueprint not found")


async def _run_workflow_async(
    execution_id: str,
    bp: Blueprint,
    inputs: Optional[Dict[str, Any]],
    store: Dict[str, _ExecutionRecord],
) -> None:
    """Background task that executes the workflow and updates *store*."""
    # Resolve workflow execution service via runtime factories when available
    try:
        from importlib import import_module

        WorkflowExecutionService = getattr(
            import_module("ice_orchestrator.services.workflow_execution_service"),
            "WorkflowExecutionService",
        )
        service = WorkflowExecutionService()
    except Exception:
        # In minimal builds orchestrator may be unavailable
        raise RuntimeError("Orchestrator runtime not available")
    record = store[execution_id]
    try:
        record["status"] = "running"
        record["_event"].set()
        # Persist running state (DB authoritative)
        try:
            async with session_scope() as session:
                rec = await session.get(ExecutionRecord, execution_id)
                if rec is not None:
                    rec.status = "running"
                    rec.started_at = _dt.datetime.utcnow()
                    await session.commit()
        except Exception:
            pass
        # Cache to Redis best-effort
        try:
            redis = get_redis()
            await redis.hset(
                _exec_key(execution_id),
                mapping={"status": "running", "blueprint_id": record["blueprint_id"]},
            )
        except Exception:
            pass

        # Attach a lightweight event emitter to reflect per-node updates into the in-memory record
        def _event_emitter(event_name: str, payload: Dict[str, Any]) -> None:
            try:
                # Minimal mapping for UI: event, node_id, status/progress
                if event_name in {
                    _Evt.NODE_STARTED,
                    _Evt.NODE_COMPLETED,
                    _Evt.NODE_FAILED,
                    _Evt.WORKFLOW_STARTED,
                    _Evt.WORKFLOW_COMPLETED,
                }:
                    record.setdefault("events", []).append(
                        {"event": event_name, "payload": payload}
                    )  # type: ignore[attr-defined]
            except Exception:
                pass

        # Execute with event emitter if the service supports it
        try:
            # Preferred path: execute from NodeSpec list
            result = await service.execute_blueprint(
                bp.nodes,
                inputs=inputs,
                name=f"run_{execution_id}",
            )
        except Exception:
            # As a last resort, construct a Workflow and execute
            from importlib import import_module

            Workflow = getattr(import_module("ice_orchestrator.workflow"), "Workflow")
            from ice_core.utils.node_conversion import convert_node_specs

            wf = Workflow(
                nodes=convert_node_specs(bp.nodes), name=f"run_{execution_id}"
            )
            result = await service.execute_workflow(wf, inputs=inputs)
        record["status"] = "completed"
        record["result"] = (
            result.model_dump() if hasattr(result, "model_dump") else result
        )  # type: ignore[assignment]
        record["_event"].set()
        # Persist completion
        try:
            async with session_scope() as session:
                rec = await session.get(ExecutionRecord, execution_id)
                if rec is not None:
                    rec.status = "completed"
                    rec.finished_at = _dt.datetime.utcnow()
                    try:
                        rec.cost_meta = (
                            result.model_dump()
                            if hasattr(result, "model_dump")
                            else {"result": str(result)}
                        )  # store output/cost meta
                    except Exception:
                        pass
                    await session.commit()
        except Exception:
            pass
        # Cache to Redis
        try:
            # Acquire a fresh Redis handle to avoid NameError if earlier cache path failed
            redis = get_redis()
            await redis.hset(
                _exec_key(execution_id),
                mapping={
                    "status": "completed",
                    "result": (
                        result.model_dump_json()
                        if hasattr(result, "model_dump_json")
                        else str(result)
                    ),
                },
            )
            try:
                import os as _os

                exec_ttl = int(_os.getenv("EXECUTION_TTL_SECONDS", "0"))
                if exec_ttl > 0 and hasattr(redis, "expire"):
                    await redis.expire(_exec_key(execution_id), exec_ttl)  # type: ignore[misc]
            except Exception:
                pass
        except Exception:
            pass
    except Exception as exc:  # noqa: BLE001
        try:
            import logging as _logging

            _logging.getLogger(__name__).error("executionFailed", exc_info=True)
        except Exception:
            pass
        record["status"] = "failed"
        record["error"] = str(exc)
        record["_event"].set()
        # Persist failure
        try:
            async with session_scope() as session:
                rec = await session.get(ExecutionRecord, execution_id)
                if rec is not None:
                    rec.status = "failed"
                    rec.finished_at = _dt.datetime.utcnow()
                    await session.commit()
        except Exception:
            pass
        try:
            # Acquire a fresh Redis handle to avoid NameError if earlier cache path failed
            redis = get_redis()
            await redis.hset(
                _exec_key(execution_id), mapping={"status": "failed", "error": str(exc)}
            )
            try:
                import os as _os

                exec_ttl = int(_os.getenv("EXECUTION_TTL_SECONDS", "0"))
                if exec_ttl > 0 and hasattr(redis, "expire"):
                    await redis.expire(_exec_key(execution_id), exec_ttl)  # type: ignore[misc]
            except Exception:
                pass
        except Exception:
            pass


# ---------------------------------------------------------------------------
# Routes
# ---------------------------------------------------------------------------


@router.post(
    "/",
    status_code=status.HTTP_202_ACCEPTED,
    dependencies=[Depends(rate_limit), Depends(require_auth)],
)
async def start_execution(  # noqa: D401 â€“ API route
    request: Request,
    payload: ExecutionStartRequest = Body(...),
    wait_seconds: float | None = Query(
        default=None,
        description=(
            "Optional: block up to N seconds and return final status/result instead of an execution_id."
        ),
    ),
) -> "ExecutionStartResponse":
    """Kick off a workflow execution.

    Expected JSON body::
        {
            "blueprint_id": "...",            # required
            "inputs": {...}                     # optional initial inputs
        }
    """

    blueprint_id = payload.blueprint_id
    inputs = payload.inputs

    # Optional project scoping: if X-Project-Id is provided, enforce that the
    # blueprint is associated with that project (added via templates panel).
    try:
        project_id = request.headers.get("X-Project-Id")
        if project_id:
            allowed_ids: list[str] = []
            # Prefer Redis authoritative list
            try:
                redis = get_redis()
                raw = await redis.lrange(f"pr:{project_id}:blueprints", 0, -1)  # type: ignore[misc]
                for r in raw:
                    if isinstance(r, (bytes, bytearray)):
                        try:
                            allowed_ids.append(r.decode())
                        except Exception:
                            continue
                    elif isinstance(r, str):
                        allowed_ids.append(r)
            except Exception:
                pass
            # Fallback to in-memory app state used by workspaces API
            try:
                if not allowed_ids:
                    store = getattr(request.app.state, "_kv", {})
                    key = f"pr:{project_id}:blueprints"
                    if isinstance(store.get(key), list):
                        allowed_ids = [str(x) for x in store.get(key, [])]
            except Exception:
                pass
            if allowed_ids and blueprint_id not in allowed_ids:
                raise HTTPException(
                    status_code=403,
                    detail="blueprint not associated with project",
                )
    except HTTPException:
        raise
    except Exception:
        # Do not block execution if scoping check fails unexpectedly
        pass

    blueprint = await _get_blueprint(request, blueprint_id)

    # Governance preflight: estimate cost and enforce budget
    try:
        from importlib import import_module

        from ice_core.utils.node_conversion import convert_node_specs

        runtime_config = getattr(
            import_module("ice_orchestrator.config"), "runtime_config"
        )
        WorkflowCostEstimator = getattr(
            import_module("ice_orchestrator.execution.cost_estimator"),
            "WorkflowCostEstimator",
        )

        node_cfgs = convert_node_specs(blueprint.nodes)
        estimator = WorkflowCostEstimator()
        est = estimator.estimate_workflow_cost(node_cfgs)
        env_budget = os.getenv("ORG_BUDGET_USD")
        budget_limit = (
            float(env_budget) if env_budget else runtime_config.org_budget_usd
        )
        if budget_limit is not None and est.total_avg_cost > budget_limit:
            raise HTTPException(
                status_code=402,
                detail=f"Estimated cost ${est.total_avg_cost:.2f} exceeds budget ${budget_limit:.2f}",
            )
    except HTTPException:
        raise
    except Exception:
        # Non-fatal if estimator fails
        pass

    exec_store = _get_exec_store(request)
    execution_id = str(uuid.uuid4())

    exec_store[execution_id] = cast(
        _ExecutionRecord,
        {
            "status": "pending",
            "blueprint_id": blueprint_id,
            "_event": asyncio.Event(),  # internal notification hook
        },
    )
    # Persist initial state to Postgres (authoritative)
    try:
        async with session_scope() as session:
            db_rec = ExecutionRecord(
                id=execution_id,
                blueprint_id=blueprint_id,
                status="pending",
                started_at=None,
                finished_at=None,
            )
            session.add(db_rec)
            await session.commit()
    except Exception:
        pass
    # Best-effort cache to Redis
    try:
        redis = get_redis()
        await redis.hset(
            _exec_key(execution_id),
            mapping={"status": "pending", "blueprint_id": blueprint_id},
        )
        try:
            import os as _os

            exec_ttl = int(_os.getenv("EXECUTION_TTL_SECONDS", "0"))
            if exec_ttl > 0 and hasattr(redis, "expire"):
                await redis.expire(_exec_key(execution_id), exec_ttl)  # type: ignore[misc]
        except Exception:
            pass
    except Exception:
        pass

    # Run in background by default. In tests (in-process TestClient), the
    # request loop may cancel orphan tasks. Allow a sync path via env toggle.
    if (
        os.getenv("ICE_EXEC_SYNC_FOR_TESTS", "0") == "1"
        or "PYTEST_CURRENT_TEST" in os.environ
    ):
        await _run_workflow_async(execution_id, blueprint, inputs, exec_store)
    else:
        asyncio.create_task(
            _run_workflow_async(execution_id, blueprint, inputs, exec_store)
        )

    # Optional synchronous waiting for simpler client UX ---------------------
    if wait_seconds and wait_seconds > 0:
        deadline = asyncio.get_event_loop().time() + wait_seconds
        # Poll minimal state until terminal or timeout
        while asyncio.get_event_loop().time() < deadline:
            state = exec_store.get(execution_id)
            status_val = state.get("status") if state is not None else None
            if status_val in {"completed", "failed"}:
                result_obj = state.get("result") if state is not None else None
                return ExecutionStartResponse(
                    execution_id=execution_id,
                    status=str(status_val),
                    result=(result_obj if isinstance(result_obj, dict) else None),
                )
            await asyncio.sleep(0.2)
        # Timed out â€“ return the id so clients can poll later
        state = exec_store.get(execution_id)
        return ExecutionStartResponse(
            execution_id=execution_id,
            status=str(
                state.get("status", "pending") if state is not None else "pending"
            ),
        )

    return ExecutionStartResponse(execution_id=execution_id, status="accepted")


@router.get(
    "/{execution_id}", dependencies=[Depends(rate_limit), Depends(require_auth)]
)
async def get_execution_status(
    request: Request, execution_id: str
) -> ExecutionStatusResponse:  # noqa: D401
    # In in-process TestClient contexts, prefer in-memory store for determinism
    # because background tasks and stubs may not reflect updates immediately.
    if (
        os.getenv("ICE_EXEC_SYNC_FOR_TESTS", "0") == "1"
        or "PYTEST_CURRENT_TEST" in os.environ
    ):
        store = _get_exec_store(request)
        if execution_id in store:
            record = store[execution_id]
            public = {k: v for k, v in record.items() if not k.startswith("_")}
            result_val: Optional[Dict[str, Any]] = None
            _rv = public.get("result")
            if isinstance(_rv, dict):
                from typing import cast as _cast

                result_val = _cast(Dict[str, Any], _rv)
            events_val: Optional[List[Dict[str, Any]]] = None
            _ev = public.get("events")
            if isinstance(_ev, list) and all(isinstance(e, dict) for e in _ev):
                from typing import cast as _cast

                events_val = _cast(List[Dict[str, Any]], _ev)
            return ExecutionStatusResponse(
                execution_id=execution_id,
                status=str(public.get("status", "unknown")),
                blueprint_id=str(public.get("blueprint_id"))
                if public.get("blueprint_id")
                else None,
                result=result_val,
                error=str(public.get("error")) if public.get("error") else None,
                events=events_val,
            )

    # DB authoritative: read status/result/events from Postgres
    async with session_scope() as session:
        row = await session.get(ExecutionRecord, execution_id)
        if row is None:
            raise HTTPException(status_code=404, detail="Execution not found")
        events_rows = (
            await session.execute(
                sa.select(ExecutionEventRecord)
                .where(ExecutionEventRecord.execution_id == execution_id)
                .order_by(ExecutionEventRecord.ts.asc())
            )
        ).scalars().all()
        events_list: List[Dict[str, Any]] = []
        for er in events_rows:
            events_list.append(
                {
                    "event": er.event_type,
                    "payload": er.payload,
                    "node_id": er.node_id,
                    "ts": er.ts.isoformat() if getattr(er, "ts", None) else None,
                }
            )
        return ExecutionStatusResponse(
            execution_id=execution_id,
            status=row.status,
            blueprint_id=row.blueprint_id,
            result=(row.cost_meta if isinstance(row.cost_meta, dict) else None),
            error=None,
            events=(events_list if events_list else None),
        )

    # Defensive default to satisfy type checker; should be unreachable
    raise HTTPException(status_code=500, detail="unreachable")


@router.get("/", dependencies=[Depends(rate_limit), Depends(require_auth)])
async def list_executions(request: Request) -> ExecutionsListResponse:  # noqa: D401
    """List executions from Postgres (authoritative)."""
    async with session_scope() as session:
        rows = (
            await session.execute(sa.select(ExecutionRecord))
        ).scalars().all()
        items = [
            ExecutionsListItem(
                execution_id=r.id, status=r.status, blueprint_id=r.blueprint_id
            )
            for r in rows
        ]
        return ExecutionsListResponse(executions=items)

    # If no session yielded, return empty list (defensive)
    return ExecutionsListResponse(executions=[])


@router.post(
    "/{execution_id}/cancel", dependencies=[Depends(rate_limit), Depends(require_auth)]
)
async def cancel_execution(request: Request, execution_id: str) -> Dict[str, Any]:  # noqa: D401
    """Best-effort cancel of a running execution.

    MVP semantics: mark status as failed with reason="canceled" and persist.
    """
    store = _get_exec_store(request)
    if execution_id not in store:
        # Update DB authoritative
        async with session_scope() as session:
            row = await session.get(ExecutionRecord, execution_id)
            if row is None:
                raise HTTPException(status_code=404, detail="Execution not found")
            row.status = "failed"
            row.finished_at = _dt.datetime.utcnow()
            await session.commit()
        # Best-effort Redis update
        try:
            redis = get_redis()
            await redis.hset(
                _exec_key(execution_id), mapping={"status": "failed", "error": "canceled"}
            )
        except Exception:
            pass
        return {"status": "canceled"}

    # Update in-memory and notify WS clients
    rec = store[execution_id]
    rec["status"] = "failed"
    rec["error"] = "canceled"
    if "_event" in rec:
        rec["_event"].set()  # type: ignore[operator]
    # Persist to Redis (best effort)
    try:
        redis = get_redis()
        await redis.hset(
            _exec_key(execution_id), mapping={"status": "failed", "error": "canceled"}
        )
    except Exception:
        pass
    return {"status": "canceled"}
